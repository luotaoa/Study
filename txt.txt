项目痛点


Agentc rag
为什么要agentic？
  - 传统RAG，第一个检索步骤无法返回任何有用结果，会崩溃
  - 加了agent，可以使用路由去判断是否应该使用RAG、应该使用哪一个RAG检索器、
逻辑：
是否要用rag？【Y--检索【检索器---包装为LangChain中的工具(tool)】，N--闲聊，人工，外部搜索】
进入rag检索后，检索出的东西是否相关（大模型评分--封装成工具）【Y--继续到生成节点，N--问题重写】
整体架构
[图片]
暂时无法在飞书文档外展示此内容
          [query]
             │
             ▼
       ⚪【上下文注入】←─(缓存/用户画像)
             │
             ▼
       ⚪【问题重写】←─(Phi-3-mini 4bit)
             │
             ▼
  ⚪【意图识别+槽位提取】←─(Joint BERT)
             │
             ▼
      ◆《置信度<阈值？》
         ╱       ╲
       是         否
        │          │
        ▼          ▼
  ⚪【澄清提问】   ◆《是否需检索？》
        │          ╱       ╲
        └───────┘         否
                          │
                          ▼
                    ⚪【闲聊/FAQ缓存】
                          │
                          ▼
                    ▭【输出答案】
                          
                          ╱
                        是
                          │
                          ▼
     ⚪【检索器+rerank+融合】←─(BGE + MiniLM + BM25)
             │
             ▼
      ◆《第一次打分通过？》
         ╱       ╲
       否         是
        │          │
        ▼          ▼
◆《重试<2次？》   ⚪【双模型生成器】←─(Phi-3-mini / Qwen-7B)
   ╱     ╲            │
 是       否           ▼
  │        │     ⚪【安全合规过滤】
  ▼        ▼           │
回重写   ▭【网络搜索后备】   ◆《合规？》
           │           ╱     ╲
           └───────┘        否
                            │
                            ▼
                     ◆《可自动改写？》
                       ╱       ╲
                     是         否
                      │          │
                      ▼          ▼
               ⚪【合规改写器】  ▭【转人工】
                      │
                      └──────────┐
                                 ▼
                         ◆《用户打分有用？》
                           ╱         ╲
                         否           是
                          │            │
                          ▼            ▼
                   ⚪【自修复尝试】   ▭【输出答案+引用】
                     ╱  │  ╲
             换模型 扩检索 加提示
                    │
                    ▼
              ◆《仍失败？》
                ╱     ╲
              是       否
               │        │
               ▼        ▼
            ▭【转人工】  ▭【输出答案+引用】
第一次打分：
相关性打分与决策 (Relevance Scoring & Decision)：
  - 流程：系统会对重排后的文档与问题的相关性进行打分。
  - 设计原因：这是第三个，也是最核心的“Agent决策点”，是系统自我反思能力的体现。它在生成答案前，先评估自己手上的资料够不够好。
  - 行动：根据分数，系统会采取三种不同的行动路径：
    - 高分路径（Happy Path）：如果相关性分数高，说明找到了高质量的参考资料。此时，系统会自信地将资料交给生成器，结合这些资料生成答案。
    - 中分路径（Self-Correction Path）：如果分数一般（二次分低），意味着内部知识库的信息可能不足或不完全相关。此时，Agent会主动调用外部工具——启动网络搜索。这极大地扩展了系统的知识边界，能回答更多时效性强或知识库未覆盖的问题。搜索到的新信息会再次进入打分环节，形成一个信息增强循环。
    - 低分路径（Rewrite Loop & Escalation Path）：
      - 首次低分：如果初次检索分数就很低，系统会触发一个查询重写循环（低分，二次重写，最大次数为2）。它会判断是不是最初的问题提得不好，尝试换一种方式重写问题再检索一次。
      - 再次低分（三次分低）：如果尝试了重写、网络搜索后，相关性分数依然很低，系统会判断自己无法回答这个问题，此时最负责任的做法就是转人工，避免强行回答导致错误。

模型选型 + 耗时
节点
推荐模型--本地部署
时间
选型原因
问题重写
 Qwen3-0.6B

150 ms


小模型速度快，支持中文，能理解上下文并标准化用户口语化表达。量化后可在 CPU 上快速推理。
意图识别 + 槽位提取

Joint BERT
（基于 bert-base-chinese 微调）
200 ms
意图与实体联合建模，参数共享，效率高；适合 To B 场景的结构化需求。也可使用 RoBERTa-wwm-ext 提升准确率。
澄清提问
规则引擎 + 小语言模型Qwen3-0.6B

80-150 ms

根据未识别的槽位生成自然语言追问，避免机械回复。生成简短追问语句，小模型
闲聊/FAQ
Qwen3-0.6B
30 - 80 ms
直接返回缓存答案，极快
检索器 + 融合重排
- 向量检索：bge-large-zh-v1.5
- 关键词检索：BM25
- Rerank：bge-m3 
- 融合策略：加权平均（向量 0.6 + 关键词 0.3）
800 ms
多模态召回提升覆盖度；Rerank 精排提升排序质量；融合避免单一方式偏差。
打分（相关性）
轻量分类器（Logistic Regression）或 DeBERTa-v3-base 微调
150 ms
不用大模型打分，降低延迟；特征包括：余弦相似度、槽位匹配度、文档时效性等。
生成器
Qwen3-8B
- 类型：因果语言模型
- 训练阶段：预训练和后训练
- 参数数量：82 亿
- 非嵌入参数数量：69.5 亿
- 层数：36
- 注意力头数（GQA）：Q 为 32 个，KV 为 8 个
- 上下文长度：原生 32,768 和 使用 YaRN 的 131,072 个令牌。
- 官方建议参数：
Temperature=0.7，TopP=0.8，TopK=20 和 MinP=0
- 推理速度：
在 H800 上可达到 60~100 tokens/s（FP16 + PagedAttention）
- 显存占用：
FP16 ≈ 16GB，INT4 ≈ 5GB →单卡可轻松部署，支持并发
- 600-1200ms
- 首句 200ms 内返回
1. 模型细节
- 60~120 tokens/s，生成 200 tokens 约 1.5~3s；
- 128K，轻松容纳 5~10 篇检索文档 + 用户历史对话；
2. 上下文管理（避免超长）
- 检索文档最多传 5 篇，每篇截取最相关 512 tokens
- 用户历史对话保留最近1~5轮（节省上下文）
- 总输入 < 模型最大上下文的 70%，留足生成空间
3. 部署---- vLLM（首选）
[图片]

安全合规过滤

正则规则库 + DeBERTa-v3-base 二分类模型
100-200 ms
规则拦截“破解”“绕过”等关键词；
分类器判断语义风险，防止“幻觉式违规”
网络搜索后备
Bing Search API
1000-1500 ms
返回 Top3 结果摘要，仅在内部知识库失败时启用，避免过度依赖外部数据。
人工介入
工单系统自动创建工单
异步创建工单，不阻塞主流程
0
标记为“高优先级”，通知技术支持团队处理，并记录反馈用于后续训练。
总体耗时--性能指标（理想路径 vs 最差情况）
场景
总耗时（ms）
是否达标
说明
理想路径（命中 FAQ / 缓存）
~300ms
✅ 极优体验
跳过重写、检索、生成
标准路径（走 RAG）
~1500ms
✅ 达标（<2s）
主流 To B 客户可接受
复杂路径（二次重写 + 网络搜索）
~3000ms
⚠️ 接近上限
可接受，但应优化
人工兜底路径
~2000ms
⚠️ 可接受
包含人工工单创建
关键优化建议（工程落地）
1. 启用缓存机制：
  - 重写后的 query 缓存（Redis）
  - 检索结果缓存（若相同问题多次出现）
  - 生成答案缓存（对常见问题）
2. 流式输出：
  - 使用 vLLM 的 stream=True，首句 200ms 内返回，提升感知速度。
3. 异步并行：
  - 意图识别 + 槽位提取 → 并行执行
  - 检索 + 网络搜索 → 并行发起，取最优结果
4. 模型量化部署：
  - 使用 GGUF / GPTQ / AWQ 对大模型进行 4-bit 量化，显著提升推理速度。
5. 埋点监控：
  - 记录每个节点耗时、成功率、错误类型
  - 建立仪表盘（Prometheus + Grafana）实时监控 SLA

---


检索后打分
1. 若文档相关：
  - 至少有一篇文档与问题相关，则进入生成环节；
  - 在具体生成之前，进行知识提炼：
    - 将相关文档再拆分为更小的知识条；
    - 对每个小知识条再次进行评分，剔除不相关的知识条，以缩短知识条的长度；
2. 若文档不相关：
  - 如果所有文档都低于相关性阈值或者评分器不确定是否相关，则框架会寻求额外的数据源：
    - 使用网络检索工具从互联网获取相关知识；
    - 对查询重写，以求检索到更相关的内容；

意图识别+槽位提取
用户的问题 可能是 多个问题 或者 复杂问题，或者有意图切换的话，传统的意图识别很难做到


1. 初级方案A：提示词工程 (Prompt Engineering)
- 从什么问题出发？
  - 最初的目标是快速实现一个能理解用户意图并提取关键信息的AI智能体。+初期收集的意图不多
  - 问题：如何让大语言模型（LLM）直接从用户输入中同时完成“判断用户想做什么”（意图）和“提取具体参数”（槽位）这两个任务？
- 为什么要这么做？
  - 这是最直接、最简单的方法。它利用了LLM强大的自然语言理解和生成能力，通过精心设计的指令（Prompt）+COT这种推理形式 ，引导模型一次性完成两个任务。
  - 目标是快速上线，成本低，无需额外的复杂系统。
- 怎么做？
  - 在一个单一的LLM调用节点中，设计一个复杂的提示词（Prompt），包含：
    1. 定义意图和槽位：明确列出所有可能的意图类别及其对应的槽位名称、类型和取值范围。
    2. Few-Shot + CoT（思维链）示例：为每个意图提供几个典型的例子（Few-Shot），并展示模型应如何一步步推理（CoT）来得出结果。
    3. 输出格式要求：严格规定输出必须是结构化的JSON格式，方便后续系统处理。
- 效果怎么样？
  - 优点：实现简单，开发速度快，对于意图数量少、场景简单的项目效果不错。
  - 缺点：当意图数量增多时，Prompt会变得非常长和复杂。模型难以在如此庞大的文本中准确聚焦于当前问题，容易出现混淆、错误关联或漏掉关键信息，导致准确率下降。维护成本也随着意图增加而急剧上升。
2. 中级方案B：意图和抽槽节点分离 (Intent & Slot Separation)
- 从什么问题出发？
  - 针对初级方案A的痛点——Prompt膨胀和准确性不足。当意图种类繁多时，单一Prompt无法有效工作。
- 为什么要这么做？
  - 将复杂的任务解耦。先专注于“判断意图”，再根据意图去“抽取对应的槽位”。这样可以将问题拆分成更小、更专注的部分，减轻单个模型的负担。
  - 提高模块化程度，便于独立维护和优化。
- 怎么做？
  - 架构上分为两个独立的LLM调用节点：
    1. 意图识别节点：只负责从用户输入中判断出是哪个预设的意图。
    2. 槽位抽取节点：根据上一步识别出的意图，调用一个专门为此意图设计的、更精简的Prompt来抽取该意图所需的所有槽位。
  - 每个意图都有一个对应的、独立的槽位抽取配置。
- 效果怎么样？
  - 优点：解决了Prompt过长的问题，逻辑更清晰，每个节点的Prompt都更短、更专注，因此准确率更高。维护性好，修改某个意图的槽位规则只需调整其对应的节点。
  - 缺点：需要两次LLM调用，增加了系统的延迟（响应时间变长）。虽然解决了单次调用的复杂度，但整体流程变慢了。
3. 进阶方案C：前置意图Rag召回 (Pre-Retrieval Intent RAG)
- 从什么问题出发？
  - 针对前两个方案的共同问题：泛化能力差。用户表达方式千变万化（方言、反问、情绪化、口语化、错别字等），标准的Prompt或少量示例无法覆盖所有情况，导致模型无法正确识别意图。
  - 例如，“难道没有坐车的码吗？” 应该被识别为“打卡乘车码”意图，但模型可能因为措辞特殊而误判。
- 为什么要这么做？
  - 利用RAG（检索增强生成）的能力，将意图识别这个任务从依赖LLM实时推理，转变为依赖预构建的知识库进行匹配和参考。
  - 核心思想是：把“泛化”的工作提前做，让模型在推理时有“参考答案”。这比单纯依赖LLM的泛化能力更可控、更准确。
- 怎么做？
  1. 构建意图知识库：人工收集或构造大量与每个意图相关的原始语料（种子语料）。
  2. 数据泛化：使用LLM对这些种子语料进行“同义句生成”，创造出成百上千种不同的、表达相同意图的提问方式（如上面的例子）。
  3. 向量化存储：将这些泛化后的query和对应的意图标签，作为文档存入Milvus或ES这样的向量数据库中。
  4. 执行流程：
    - 用户提问 → RAG召回：将用户的新提问，去向量数据库中检索最相似的那些历史query。
    - 返回结果：RAG召回的结果（即最相似的历史query及其对应的意图）作为“案例”或“上下文”，附加到LLM的Prompt中。
    - LLM处理：LLM基于这些“案例”进行推理，从而更准确地判断当前用户的意图。
- 效果怎么样？
  - 优点：极大地提升了意图识别的准确性和鲁棒性，特别是对非标准、多样化的用户表达。当线上出现新的、未被覆盖的query时，只需将其添加到知识库中，就能快速修复，无需重新训练或修改模型。成本相对可控（主要在前期泛化）。
  - 缺点：增加了前期的数据准备和泛化工作量。对于需要综合多轮对话信息才能判断意图的场景，效果不佳。
4. 高阶方案D：合并意图抽槽节点 + 升级前置Rag召回能力
- 从什么问题出发？
  - 针对方案C的局限性：无法处理多轮对话。在实际交互中，用户可能会分步提问，比如：“我要打车” → “去外滩” → “我在陆家嘴”。之前的方案C只能看单轮，无法利用历史信息。
  - 同时，方案B存在延迟问题。
- 为什么要这么做？
  - 目标是在保证高准确率的同时，降低延迟，并支持多轮对话。这是一个综合性挑战。
  - 核心思路是：利用RAG的强项，将整个“意图+槽位”的解析过程，变成一个基于知识库的、高效的检索-生成过程。
- 怎么做？
  1. 构建完整的CASE知识库：不再仅仅是“query -> intent”的映射，而是构建包含完整对话场景的“案例”（Case）。每个Case是一个结构化的样本，包含：
    - 历史提问 (History)
    - 最新提问 (Latest Query)
    - 思考过程 (Reasoning Process - 可选，用于指导模型)
    - 意图 (Intent)
    - 槽位 (Slots)
    - 处理 (Handling: 如“直接回答”或“意图路由”)
  2. 升级RAG召回：
    - 组合查询：在用户发起新提问时，将多轮历史对话和当前最新提问拼接起来，形成一个完整的“上下文查询”。
    - 精准召回：用这个组合后的查询去知识库中检索最相似的CASE。
  3. 智能决策：
    - 如果召回的CASE的“处理”字段是“直接回答”，则直接返回该CASE中的预设答案，跳过LLM，极大降低延迟。
    - 如果是“意图路由”，则将召回的CASE作为上下文，输入给LLM，由LLM进行最终的意图和槽位判断。
  4. 意图切断策略：为了防止旧话题干扰新话题，当检测到意图切换时，主动mask旧意图的历史记录。
- 效果怎么样？
  - 优点：这是目前最成熟、最强大的方案。它完美融合了RAG的高效性与LLM的灵活性，实现了：
    - 高准确率：利用丰富的CASE库进行精准召回。
    - 低延迟：通过“直接回答”机制，避免不必要的LLM调用。
    - 支持多轮对话：能够利用历史信息进行综合判断。
    - 可维护性强：所有逻辑都在知识库中，更新快。
  - 缺点：前期需要投入大量精力来构建高质量的CASE库，工作量最大。

5.微调
前面方案的问题：
- 需要做数据预处理，有一定开发成本；
- 知识库内容较多或质量不佳时，可能引起模型幻觉和分类冲突；
- 相比方案1，会增加向量召回部分的延迟，且模型要求依然在14b以上，有一定延迟问题；
通过模型训练来增强数据
- 全参数微调 OR 轻量化微调（LoRA和QLoRA）
全参数微调消耗计算资源最多，而且容易使大模型产生灾难性遗忘，LoRA和QLoRA有效地避免了这个问题。另一方面，QLoRA由于参数精度低，容易对下游任务的效果产生不利影响。综合考虑，使用LoRA算法进行微调。
全参数微调会修改模型中所有权重，而大模型权重高度共享且互相关联，新任务梯度可能破坏原有知识，导致灾难性遗忘。
- 全局批次大小
全局批次大小=卡数*per_device_train_batch_size*gradient_accumulation_steps
这里在GPU显存允许的情况下尽可能调大batch size，可以使得模型更快收敛到最优解，同时具有较高的泛化能力。
- 序列长度
序列长度对显存消耗和训练效果有较大的影响，过小的序列长度虽然节省了显存，但是导致某些比较长的训练数据集被切断，造成不利影响；过大的序列长度又会造成显存的浪费。从意图识别的场景来看，根据实际数据的长度，选择64/128/256的长度比较合适。
- 学习率
如果训练数据质量比较差，训练效果一般会受影响，所以在数据标注的时候需要进行充分的质量校验。同时，由于LoRA训练一般参数调整空间不大，学习率默认可以进行偏大设置，例如1e-4左右，当训练loss下降过慢或者不收敛时，建议适当调大学习率，例如3e-4或者5e-4。不建议使用1e-3这个量级的学习率，容易得不到优化的结果。
- 模型选择
一般而言，模型底座越大，下游任务效果越好，但是部署成本和推理代价相应增大。针对意图识别的场景，建议从4B左右的大模型底座开始进行SFT和调参，当效果较大同时通过调参无法进一步提升时，建议换成7B的更大底座。超过10B的底座理论上能得到更好的结果，但是需要权衡实际的效果和成本问题，因此，因此本场景使用7B的底座性价比较高。
（5）模型离线评测
当模型训练结束后，可以使用如下Python脚本进行模型效果的评测。假设评测数据如下：
[    
    {        "instruction":"想知道的十年是谁唱的？",                    
              "output":"music_query_player(十年)"    
     },    
    {        "instruction":"今天北京的天气怎么样？",            
                "output":"weather_search(杭州)"         
     }
]



RAG 细节
1. 文件概要【全英文的文件】
  1. 文件类型：
    1. 技术文档、产品文档、操作手册、
    2. 签约条款
    3. 历史QA <-- 【历史工单、邮件】
  2. 文件格式：pdf、doc -->md
  3. 文件份数：
    1. 2k PDF 
    2. 500 doc 【100份  签约条款】
    3. 1k QA md文档【人工制作】
2. QA 文档 人工制作【没那么重要】
- PII脱敏 
  思路：采用分层脱敏策略，确保数据合规且高效处理纯英文数据。
  - 结构化字段脱敏：使用正则表达式（Regex）精准匹配常见PII，如邮箱（[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}）、电话号码（\+?\d{10,12}）、API密钥等，替换为统一占位符（如[EMAIL]、[PHONE]）。
  - 非结构化实体识别：结合spaCy的预训练英文NER模型（en_core_web_lg）识别姓名、公司、地址等实体，补充正则无法覆盖的场景。进一步用Hugging Face的distilbert-base-cased NER模型（轻量且高效）进行校验，提高识别准确率。最终将所有PII替换为通用占位符（如[ENTITY]），确保一致性。
- QA抽取
  思路：利用大语言模型高效提取对话中的核心问题和解决方案，输出结构化JSON。
  - 模型选择：使用Grok 3（xAI自研模型，性能优异且支持强约束Prompt）提取QA对，避免依赖外部模型如GPT-4或Claude 3。
  - Prompt设计：设计强约束Prompt，明确要求模型输出JSON格式，包含"core_question"（客户问题，简洁且具检索语义）和"solution"（最终有效解决方案）。约束包括：问题需提炼为独立语句（<50字），解决方案需具体且可操作，剔除无关对话内容。
  - 后处理：对输出JSON进行格式校验（如JSON Schema），确保字段完整性和语义清晰，必要时人工审核边缘案例。
- 质量过滤与去重
  思路：多阶段过滤与去重，兼顾效率和语义准确性。
  - 规则过滤：基于规则剔除无效数据，如空答案、文本长度<10字、标记为“未解决”的工单。
  - 文本去重：使用MinHash+LSH（局部敏感哈希）快速检测近似重复文本，设置Jaccard相似度阈值0.9，剔除高重复内容。
  - 语义去重：对剩余文本用Sentence-BERT（all-mpnet-base-v2）生成语义向量，结合HDBSCAN聚类识别语义相似的QA对，选取每簇中最具代表性的样本（基于向量中心或最长文本）。
  - 优化：对聚类结果进行抽样检查，确保保留问题多样性。

3. 文件解析【！！！】
3.1 Pdf 解析 pipeline
    - 解析工具：minerU【一般准 但处理不了跨页表格】、textln【特别准 但是要钱】
    - 解析思路：
      1. 分流
        1. 识别跨页表格文件
          MinerU 同时生成 JSON 文件（包含页面信息）和 Markdown 文件。
          [ MinerU 的 表格处理机制 DocLayout-YOLO 和 SLANet_Plus 算法会将一个跨页表格的全部内容存储在 同一个 JSON 文件 的单个表格元素中，而不是拆分成多个 JSON 文件或多个表格元素。]
          如果 JSON 文件检测到跨页表格，但 Markdown 输出中： 
            - md表格数量[正则匹配] 多于 json跨页表格的预期数量（说明未合并）。
            - 或者表格行数太少<3（说明内容不完整）。 则认为该文件解析不佳。
        2. 判断是 pdf 是不是扫描件：magic_pdf\filter\pdf_classify_by_type.py  
      2. 主解析-minerU
        - 硬件配置：
          - 硬件：多GPU服务器（RTX4090 * 4）
          - 后端：vlm-sglang-client + server 集群
          - 架构：分布式部署
          - 性能预期：1000+页/小时，3.6s/页   [4090 24g 单卡是10s/份]
        - minerU简单原理介绍：
          - Magic-pdf 包
          - MinerU 依赖 PDF-Extract-Kit 模型集合，通过多个专用模型协同完成 PDF 解析任务。
          - 布局分析：layoutlmv3或doclayout_yolo 
          - OCR处理、表格识别：百度的 PaddleOCR
          - [参考]深入拆解 MinerU 解析处理流程：https://mineru.site/%E6%80%9D%E7%BB%B4%E9%AB%98%E5%9C%B0/2025/05/22/1fb9e6fa-6062-8053-babf-e5b0709ffd74
      3. 二次解析--TextIn 处理问题文件
    
3.2 文档数据清洗：【不重要，但在流程里】
  遇到的问题：
  - 标题混乱
  - 图片的标注和链接被拆分成两行
  - 多余的空白符/换行
  - ....
  [大模型清洗/根据具体问题清洗]
  
  
4. chunk
4.1 痛点：langchain的语义、递归分块效果不好
4.2 三层机制：
    建立DOM文档树、
    分类block+语义增强+上下文绑定与知识链接、
    语义边界检测+长度限制
4.3 【结构化构建】建立DOM文档树
  - 输入：Markdown
  - 步骤：
    - 使用 mistune+正则  → AST → DOM 树
    - 章节子树切分（H1/H2/H3）。
    - 保留各种元素类型：文本、表格、图片、代码、列表、步骤。
    - 附加位置索引（行号、字符范围）。
  - 目标：
    - 在保持逻辑完整性的同时，形成第一批结构单元
    - 输出：结构化文档树 + 位置索引
4.4 【语义增强】分类block+多模态语义增强+上下文绑定与知识链接
  1. 分类block：文本、代码、表格、图片、列表等。
  2. 多模态语义增强：
  - 图片节点 (Image Node)：
    - 调用视觉语言模型（VLM，qwen-2.5VL），为图片生成两种描述，并存入节点元数据：
      - vlm_objective_description: 对内容的客观描述（“图中是一个流程图，包含A、B、C三个方框...”）。
      - vlm_inferential_summary: 对图片意图的推断（“该流程图展示了用户身份验证的三步过程...”）。
  - 表格节点 (Table Node)：
    - 调用LLM，生成摘要table_summary: 对表格内容的自然语言总结（“此表对比了产品A、B、C在性能、价格、兼容性上的差异...”）。
  - 
4.5 【Chunk 标准化与索引预处理】语义边界检测+长度限制
  经过第二层处理，语义块太大或太“裸” → 需要切分+打标+适配检索系统
  
  
  
4.6 细节：
  chunk数量：
  每个chunk的token数量：
  chunk效果：
5. embedding
5.1 选型：bge-m3
    - 稀疏嵌入、密集嵌入和多向量嵌入（ColBERT 风格）
    - XLM-RoBERTa 架构
    - 维度：密集向量维度为 1024
  
5.2 微调：
why？
  微调的核心目标是进行度量学习（Metric Learning）：
  - 拉近语义相似/相关的文本对（如“查询-答案”）在向量空间中的距离。
  - 推远语义不相似/不相关的文本对在向量空间中的距离。
  - 通过优化如三元组损失（Triplet Loss） 或 InfoNCE损失 等函数，使模型在特定场景下的相似度计算、信息检索等任务上表现更精准。
how？
  训练的核心是对比学习（Contrastive Learning）。
  训练流程简述：
  1. 准备数据：构建包含query、pos（正样本）、neg（负样本）的数据集（通常是JSON Lines格式）。
  2. 模型初始化：加载预训练的BGE-M3模型权重。
  3. 前向计算：将query、pos、neg输入模型，得到各自的Embedding向量。
  4. 计算损失：使用 InfoNCE损失函数 计算损失。该函数的核心思想是最大化query与pos的相似度，同时最小化query与所有neg的相似度。
    - L = -log[ exp(s(q, p)/T) / (exp(s(q, p)/T) + Σ exp(s(q, ni)/T)) ]
    - 其中，s()是相似度函数（如余弦相似度），T是温度参数。
  5. 反向传播：根据损失计算梯度，更新模型参数。
  6. 迭代：重复步骤3-5，直到模型收敛或达到预定训练轮数。
  关键训练技术：
  - 批内负样本（In-batch Negatives）：在同一个训练批次中，将其他样本的正例（甚至查询）作为当前样本的负例。这是最常用且高效的方法。
  - 难负样本（Hard Negatives）：专门挖掘那些语义上与查询相似但实际不相关的样本，迫使模型学习更精细的区分能力。
配置：
  4090，4张24G 
[图片]
5.3 微调数据构建：
  准备方式：
  1. 数据源评估：选择能反映目标应用场景的原始数据（如用户日志、问答对、知识库文档）。
  2. 数据清洗：去除无效字符、处理缺失值、统一编码。
  3. 字段映射：将原始数据的字段映射到目标格式。例如，将原始数据的question列作为query，将answer或context列作为pos。
  4. 构建正样本：确保pos与query强相关。可以是直接答案，也可以是包含答案的上下文段落。
  5. 构建负样本：这是最关键也最复杂的一步（详见下一节）。
  6. 添加Prompt：为query添加合适的指令，如"Represent this sentence for searching relevant passages:"。这能显著提升BGE模型在检索任务上的表现。
  7. 数据集划分：将数据按比例（如8:1:1）划分为训练集、验证集和测试集。
  8. 难负样本：
  难负样本（Hard Negatives）是指那些语义上与查询很相似，容易被模型误判为正例，但实际上并不相关的样本。它们对提升模型的细粒度辨识能力至关重要。
  挖掘策略：
  1. 基于稀疏检索召回（如BM25）：
    - 对每个query，使用BM25算法在整个语料库中检索出Top-K个最相似的文档。
    - 移除其中的真实正例。
    - 剩下的文档就是高质量的难负样本候选集，因为它们在词汇层面与查询高度重合。
  2. 利用Embedding模型筛选：
    - 使用一个预训练的Embedding模型（甚至可以是正在微调的模型本身）计算query与语料库中所有文档的余弦相似度。
    - 选择那些相似度得分很高但并非真实正例的文档作为难负样本。
    - 进阶方法：使用更精细的Cross-Encoder模型对候选负样本进行重排序，选出最难区分的样本。
  3. 结合领域知识或规则：
    - 在特定领域，可以人工或半自动地构造难负样本。例如：
      - 在金融领域，将“市盈率”的查询与“市净率”、“市销率”等概念的解释配对。
      - 在电商领域，将同一品类但不同品牌、不同规格的产品描述配对。
6. 索引存储

7. 检索前处理
7.1 查询重写
7.2 假设性答案
8. 检索中
指定 k（top-k，通常 3-20）；可选阈值过滤（e.g., 相似度 > 0.7）
检索
索引

Milvus 摘要检索
IVFFLAT
L2距离
Milvus 全文检索
HNSW
COSINE
ES 检索

定义这个 Index 的 Mapping（模式），告诉 ES 每个字段是什么类型，以及搜索时如何对待它。
  chunk_id: keyword 类型（不分词，用于精确匹配）。
  content: text 类型（会进行分词，用于全文搜索）。
  entities: text 类型，并设置一个很高的权重（boost），比如 5。
  hierarchy_path: text 类型，设置一个中等权重，比如 2。
  三路召回:
  - Path A: 摘要级别检索 
    步骤1: 在 Milvus group_summaries_collection 中进行向量搜索，找到最相关的章节摘要，例如命中了 2.1 的摘要节点。
    步骤2: 获取该摘要节点的 node_id (node_id_of_2.1)。
    步骤3: 在 Milvus content_chunks_collection 中进行第二次向量搜索，但增加一个元数据过滤条件：parent_node_id == 'node_id_of_2.1'。这样就在 2.1 章节内部找到了最匹配的 chunk2 和 chunk3。
    输出: 高度相关的 content_chunk 列表。
  - Path B: 内容块向量检索 (自由探索 + 上下文扩展)
    步骤1: 直接在 Milvus content_chunks_collection 中进行全局向量搜索，可能会找到来自不同章节的零散但相关的 content_chunk，例如命中了 2.1 的 chunk2 和 3.5 的 chunk1。
    输出: 语义相关的 content_chunk 列表。
  - Path C: 关键词检索
    步骤1: 在 Elasticsearch 中执行 BM25 关键词查询。
    输出: 关键词匹配的 content_chunk 列表。
  

---
  
9. 检索后处理
  去重：合并列表后，移除重复文档（基于 ID 或内容相似度）
  摘要--IVF FLAT---L2距离
  全文--HNSW--cosine
  ES---
  一路30,3路一共90
  
 融合 (Fusion)
9.1 融合、去重

---
  如果使用多检索器，合并结果（如加权平均分数）；去除重复 chunk。
  融合与最终处理:
  融合 (Fusion): 将三路召回的 content_chunk 列表使用 RRF算法进行合并、去重、排序。
  RRF 是一种无监督、无需训练、对排序鲁棒的融合算法，特别适合异构排序列表融合。其核心思想是：一个文档在多个列表中排名越靠前，其综合得分越高。
  公式：RRF_score(doc) = Σ_{每个召回路径} ( 1 / (k + rank_i(doc)) )
  - rank_i(doc)：文档 doc 在第 i 路召回结果中的排名（从1开始）
  - k：常数，通常取60（经验值，可调），用于平滑排名靠后项的贡献
示例：
  假设某 chunk2 出现在：
  - Path A 中排名第1 → 得分 = 1/(60+1) ≈ 0.0164
  - Path B 中排名第3 → 得分 = 1/(60+3) ≈ 0.0159
  - Path C 中未出现 → 得分 = 0
  → 总 RRF_score ≈ 0.0323
  另一个 chunk5 出现在：
  - Path B 第1 → 0.0164
  - Path C 第2 → 0.0161
→ 总分 ≈ 0.0325 → 最终排名更高
最终RRF分数胜出的chunk，上下文扩展: 
  如 chunk2，查询 MongoDB 找到其父节点 2.1，然后获取 2.1 下所有的兄弟节点 (chunk1, chunk2, chunk3) 作为最终要提供给 LLM 的上下文。这完美实现了你“返回整个2.1”的需求。

---
9.2 上下文压缩：
  精简检索结果，保留关键句子（e.g., 使用 LLM 总结或提取高相关片段），避免提示过长。
9.3 可选：
  置信度评估，如果分数低则触发再检索或 fallback 到纯生成。
10. 评估
10.1 Ragas评估工具
  https://mp.weixin.qq.com/s/k9PaBh4FZBHeT52uqWqg3A
    1、评估数据
    为了评估RAG系统，Ragas需要以下信息:
    - question：用户输入的问题。
    - answer：从 RAG 系统生成的答案(由LLM给出)。
    - contexts：根据用户的问题从外部知识源检索的上下文即与问题相关的文档。
    - ground_truths： 人类提供的基于问题的真实(正确)答案。 这是唯一的需要人类提供的信息。
    准备数据：question 和 ground_truths 的配对，1000条
    
    2、评估指标
    - 忠实度(faithfulness)
      - 如果答案(answer)中提出的所有基本事实(claims)都可以从给定的上下文(context)中推断出来，则生成的答案被认为是忠实的。为了计算这一点，首先从生成的答案中识别一组claims。然后，将这些claims中的每一项与给定的context进行交叉检查，以确定是否可以从给定的context中推断出它。
    - 答案相关性(Answer relevancy)
      - 答案(answer)与用户问题(question)之间相关程度 [0,1]
    - 上下文精度(Context precision)
      - 评估所有在上下文(contexts)中呈现的与基本事实(ground-truth)相关的条目是否排名较高。理想情况下，所有相关文档块(chunks)必须出现在顶层。该指标使用question和计算contexts，值范围在 0 到 1 之间，其中分数越高表示精度越高。
[图片]
    - 上下文召回率(Context recall)
      - 上下文召回率(Context recall)衡量检索到的上下文(Context)与人类提供的真实答案(ground truth)的一致程度。它是根据ground truth和检索到的Context计算出来的，取值范围在 0 到 1 之间，值越高表示性能越好。
    - 上下文相关性(Context relevancy)
      - 据用户问题(question)和上下文(Context)计算得到，并且取值范围在 (0, 1)之间，值越高表示相关性越好。理想情况下，检索到的Context应只包含解答question的信息。 我们首先通过识别检索到的Context中与回答question相关的句子数量来估计 |S| 的值。
  
10.2 BC 边界清晰度
    如果边界不清晰，嵌入向量可能捕捉到跨分块的噪声，导致召回率低或生成答案不准。BC 帮助优化分块策略（如固定长度 vs. 语义分块），并与 RAG 性能（如 ROUGE-L 分数）呈正相关。
    困惑度（ppl）：语言模型评估文本流畅性和可预测性的指标。
    ppl(q)：句子序列 q 的困惑度，表示模型对文本的理解程度。
    ppl(q|d)：在给定上下文 d 下的对比困惑度。
[图片]
def compute_perplexity(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():  # 不训练，只预测
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss  # 模型的“错误率”
        ppl = torch.exp(loss)  # ppl = e^loss（指数，变“难易分数”）
    return ppl.item()  # 返回数字
    
输入一段文本（text），用 tokenizer（分词器，像把句子切成词的“刀”）把文本变成模型能懂的数字序列（inputs）。然后喂给模型（model），模型假装“预测”这个文本的每个词（用 labels=输入本身，计算它自己“重现”文本的准确率）。
    BC 不是简单比较字符串相似度（如 cosine similarity），而是评估“模型在不同上下文下的理解难度差异”。预训练模型如 GPT-2（自回归生成模型）或 BERT（双向 masked 语言模型）在海量语料上训练，能捕捉： 
    - 上下文依赖：例如，合并两个分块后，如果“it”在 cic_ici 中指代 A，但在 ci+1c_{i+1}ci+1 中指代 B，模型会产生高 ppl，因为上下文冲突。这反映真实 RAG 问题（如检索时语义混淆）。
    - 自然语言流畅性：PLM 知道“句子边界”或“主题转移”的概率模式，而非手工规则（如仅看标点）。
    - 为什么不是简单方法？如果不用 PLM，只用 TF-IDF 或 n-gram 统计，ppl 计算会退化为浅层统计，无法处理复杂语义（如同义词、隐喻），导致 BC 指标失真。研究显示，这种基于 PLM 的 ppl 对比更能预测 RAG 性能（如召回率提升 5-10%）。
    - 
    - BC > 0.8 表示优秀分块；< 0.5 表示需优化。
  
10.3 MMR 答案定位效率，（平均倒数排名）
  衡量检索系统中第一个相关文档的排名位置。
  假设有3个查询： 
  - 查询1：正确文档排在第1位 → 倒数排名 = 1/1 = 1
  - 查询2：正确文档排在第2位 → 倒数排名 = 1/2 = 0.5
  - 查询3：正确文档排在第5位 → 倒数排名 = 1/5 = 0.2
  - MRR = (1 + 0.5 + 0.2) / 3 ≈ 0.57
  LangChain 的 max_marginal_relevance_search  
  查询（Query）：用户输入的文本字符串，用于计算相关性。
  候选文档集（Candidate Documents）：从知识库中初始检索的文档列表（通常是向量化的文本块），作为 MMR 的“选择池”。
  嵌入模型（Embeddings）：用于将查询和文档转为向量，实现相似度计算（Sim1 和 Sim2）。
  参数：如 λ（平衡因子）、k（输出文档数）、fetch_k（初始候选数）。

QPS1200 怎么圆？
- 并发量（Concurrency）：同时处理的请求数，比如 100 个用户同时发请求。
- QPS（吞吐量）：每秒成功处理的请求数，比如 1 秒内处理完 1200 个请求。
“当时写的是系统在单机 100 并发压力下，实测 QPS 达到 1200。这得益于我们做了几项优化： 
1. 使用 vLLM 的 PagedAttention + Continuous Batching，极大提升 GPU 利用率；
2. 所有模型 4-bit 量化，推理延迟 < 10ms；
3. 缓存命中率 > 80%，简单请求走 Redis，不进大模型。
所以虽然并发不高，但因为每个请求处理极快，整体吞吐能达到 1200 QPS。”

---


问题：
问题进入系统的流程是怎么样的？



你知道哪些其他的chunk方式？
1. 固定大小（Fixed-Size） → 简单的基于 token/字符的分割
2. 递归切分（Recursive） → 按分隔符尊重文档结构
3. 基于文档（Document-Based） → 按标题、代码块、HTML 标签分割
4. 语义切分（Semantic） → 基于嵌入的语义驱动切分
5. LLM 切分（LLM-Based） → 让模型定义边界
6. 代理式切分（Agentic） → AI 智能体动态选择最佳方法
7. 延迟切分（Late Chunking） → 先嵌入，再切分（保留完整上下文）
8. 层级切分（Hierarchical） → 多层次切分（先宏观再细节）
9. 自适应切分（Adaptive） → 根据内容密度动态调整大小与重叠度

开放：为什么不用开源的rag框架？
LightRAG的优缺点
1、解决的问题：
全面信息理解：通过多跳子图的全局信息提取，能在多领域复杂查询中表现出色。
检索效率高：相比传统文本分块遍历方法，利用图和向量的集成，大幅减少检索时间和计算开销。
快速适应新信息：增量更新算法使系统能及时整合新数据，保持实时性和有效性。
2、缺点：
检索方式：无法自动判断query为细节问题或抽象性问题，只有使用LLM才能做出自动化判断。
3、与GraphRAG的不同点：
架构设计：LightRAG专注于图结构的向量化实现，避免了GraphRAG的社区遍历。
检索方式：LightRAG采用双层检索框架，通过向量化的关键词匹配，实现局部和全局信息的高效整合，降低了计算和存储开销。

为什么使用三个数据库？为什么选mongoDB？
