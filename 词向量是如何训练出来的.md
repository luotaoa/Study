# 词向量是如何训练出来的？

## 🎯 核心答案

**词向量不是人为规定的，而是通过大量文本数据训练出来的！**

AI通过"看"几十亿个句子，自己学会了把相似的词放在相似的位置。

---

## 🤔 先理解问题

### 你的疑问

> "这些向量都是人为规定的吗，还是训练出来的？"

**答案：完全是训练出来的！** 人类只提供：
- ✅ 大量文本数据
- ✅ 训练算法
- ❌ **不**手动规定任何词的向量值

---

## 🧠 核心思想：分布式假设

### "物以类聚，人以群分"

**核心原理：** 经常出现在相似上下文中的词，意思往往相似

```
例子1:
- "我养了一只猫"
- "我养了一只狗"
→ "猫"和"狗"经常出现在相同位置 → 它们相似（都是宠物）

例子2:
- "国王住在城堡里"
- "女王住在宫殿里"
→ "国王"和"女王"经常出现在相似语境 → 它们相似（都是君主）

例子3:
- "学习Python编程"
- "学习JavaScript编程"
→ "Python"和"JavaScript"上下文相似 → 它们相似（都是编程语言）
```

---

## 📚 训练方法1: Word2Vec（最经典）

### 由Google在2013年发明

### 两种训练方式

#### 方式1: CBOW (Continuous Bag of Words)

**用周围的词预测中间的词**

```
训练数据: "我爱学习人工智能"

训练样本:
输入: ["我", "爱", "人工", "智能"]  ← 上下文
输出: "学习"                        ← 目标词

模型学习: 
"当周围出现'我爱...人工智能'时，中间很可能是'学习'"
```

**详细过程：**

```
句子: "我 爱 学习 人工 智能"
      ↓  ↓   ↓   ↓   ↓
     w1 w2  w3  w4  w5

生成训练样本（窗口大小=2）:

样本1: 预测 w1 (我)
  输入: [w2, w3] = ["爱", "学习"]
  输出: w1 = "我"

样本2: 预测 w2 (爱)
  输入: [w1, w3, w4] = ["我", "学习", "人工"]
  输出: w2 = "爱"

样本3: 预测 w3 (学习)
  输入: [w1, w2, w4, w5] = ["我", "爱", "人工", "智能"]
  输出: w3 = "学习"

... 以此类推
```

---

#### 方式2: Skip-gram

**用中间的词预测周围的词**（Word2Vec默认方法）

```
训练数据: "我爱学习人工智能"

训练样本:
输入: "学习"                        ← 中心词
输出: ["我", "爱", "人工", "智能"]  ← 上下文词

模型学习:
"当中心词是'学习'时，周围常出现'我爱...人工智能'"
```

**详细过程：**

```
句子: "我 爱 学习 人工 智能"
         ↓  ↓   ↓   ↓   ↓

中心词: "学习" (窗口大小=2)

生成训练对:
("学习", "我")    ← 学习 → 我
("学习", "爱")    ← 学习 → 爱
("学习", "人工")  ← 学习 → 人工
("学习", "智能")  ← 学习 → 智能

换下一个中心词: "人工"
("人工", "爱")
("人工", "学习")
("人工", "智能")

... 遍历所有词
```

---

## 🔢 神经网络结构（Skip-gram示例）

### 简化版架构

```
输入层          隐藏层(向量层)      输出层
  ↓               ↓                ↓
┌─────┐        ┌─────┐          ┌─────┐
│ 学习 │        │     │          │  我  │
│     │───────→│     │─────────→│ 爱   │
│(one-│        │     │          │ 人工 │
│hot) │        │     │          │ 智能 │
└─────┘        └─────┘          └─────┘
10000维         300维           10000维
(词表大小)    (词向量维度)     (词表大小)

这个300维的隐藏层，就是我们要的词向量！
```

### 详细计算过程

```python
# 假设词表大小=5（实际是几万到几十万）
词表 = ["我", "爱", "学习", "人工", "智能"]
词表大小 = 5
向量维度 = 3  # 简化演示，实际是100-300

# 输入: "学习" 的 one-hot 编码
input = [0, 0, 1, 0, 0]  # 第3个位置是1，表示"学习"

# 权重矩阵 W1: 5×3 (初始随机值)
W1 = [
    [0.1,  0.2,  0.3],  # "我"的向量（会被训练调整）
    [0.4,  0.5,  0.6],  # "爱"的向量
    [0.7,  0.8,  0.9],  # "学习"的向量 ← 这就是我们要的！
    [0.2,  0.3,  0.4],  # "人工"的向量
    [0.5,  0.6,  0.7]   # "智能"的向量
]

# 隐藏层 = input @ W1
# 因为 input 是 one-hot，所以结果就是取出对应行
hidden = [0.7, 0.8, 0.9]  # "学习"的向量

# 输出层: hidden @ W2 → 预测概率分布
# W2: 3×5
output = [0.1, 0.2, 0.5, 0.15, 0.05]  
# 表示预测周围词的概率
# "学习"周围最可能出现"学习"本身(50%)

# 真实标签（实际周围词）
target = [1, 1, 0, 1, 1]  # "我爱人工智能"都应该是1

# 计算损失，反向传播，更新W1和W2
# 训练几百万次后，W1中的每一行就是训练好的词向量！
```

---

## 🎓 训练过程（一步一步）

### 第1步: 准备数据

```python
# 收集大量文本
corpus = [
    "我爱学习编程",
    "Python是编程语言",
    "我喜欢学习Python",
    "编程让我快乐",
    # ... 几十亿句话
]

# 建立词表
vocab = {"我": 0, "爱": 1, "学习": 2, "编程": 3, ...}
vocab_size = len(vocab)  # 假设50000个词
```

---

### 第2步: 生成训练样本

```python
# Skip-gram: 中心词 → 上下文词
def generate_training_data(sentence, window_size=2):
    words = sentence.split()
    samples = []
    
    for i, center_word in enumerate(words):
        # 获取上下文词
        start = max(0, i - window_size)
        end = min(len(words), i + window_size + 1)
        
        for j in range(start, end):
            if i != j:  # 不包括中心词自己
                context_word = words[j]
                samples.append((center_word, context_word))
    
    return samples

# 示例
sentence = "我爱学习编程"
samples = generate_training_data(sentence, window_size=2)

# 生成的训练对:
# ("我", "爱"), ("我", "学习")
# ("爱", "我"), ("爱", "学习"), ("爱", "编程")
# ("学习", "我"), ("学习", "爱"), ("学习", "编程")
# ("编程", "爱"), ("编程", "学习")
```

---

### 第3步: 初始化神经网络

```python
import numpy as np

vocab_size = 50000     # 词表大小
embedding_dim = 300    # 向量维度

# 随机初始化权重矩阵
W1 = np.random.randn(vocab_size, embedding_dim) * 0.01  # 输入→隐藏
W2 = np.random.randn(embedding_dim, vocab_size) * 0.01  # 隐藏→输出

# W1的每一行就是一个词的向量（初始是随机的）
print(W1[vocab["学习"]])  # [0.002, -0.001, 0.003, ...]
```

---

### 第4步: 训练（核心！）

```python
# 超参数
learning_rate = 0.01
epochs = 10

# 训练循环
for epoch in range(epochs):
    for center_word, context_word in training_samples:
        
        # 1. 前向传播
        # 获取中心词的one-hot编码
        center_idx = vocab[center_word]
        context_idx = vocab[context_word]
        
        # 隐藏层 = 中心词的向量
        h = W1[center_idx]  # (embedding_dim,)
        
        # 输出层 = 预测上下文词的概率
        u = h @ W2  # (vocab_size,)
        y_pred = softmax(u)  # 转换为概率分布
        
        # 2. 计算损失
        # 真实标签: context_word应该概率为1
        loss = -np.log(y_pred[context_idx])
        
        # 3. 反向传播
        # 计算梯度
        y_pred[context_idx] -= 1  # 梯度
        
        # 更新W2
        grad_W2 = np.outer(h, y_pred)
        W2 -= learning_rate * grad_W2
        
        # 更新W1（中心词的向量）
        grad_h = y_pred @ W2.T
        W1[center_idx] -= learning_rate * grad_h

# 训练完成！W1就是词向量矩阵
word_vectors = W1

# 现在每个词都有了向量
print("'学习'的向量:", word_vectors[vocab["学习"]])
print("'编程'的向量:", word_vectors[vocab["编程"]])
```

---

### 第5步: 神奇的事情发生了！

```python
# 训练完成后，相似的词会有相似的向量

# 向量相似度计算
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# 计算相似度
vec_学习 = word_vectors[vocab["学习"]]
vec_编程 = word_vectors[vocab["编程"]]
vec_苹果 = word_vectors[vocab["苹果"]]

print(cosine_similarity(vec_学习, vec_编程))  # 0.85 (相似！)
print(cosine_similarity(vec_学习, vec_苹果))  # 0.23 (不相似)
```

---

## 🎨 为什么相似的词会有相似的向量？

### 直观理解

```
训练过程中:

句子1: "我爱学习编程"
→ 模型学到: "学习"周围经常出现"编程"

句子2: "我喜欢学习Python"
→ 模型学到: "学习"周围经常出现"Python"

句子3: "编程让我快乐"
→ 模型学到: "编程"周围经常出现"我"

句子4: "Python让我快乐"
→ 模型学到: "Python"周围经常出现"我"

神经网络发现:
- "编程"和"Python"经常出现在相同的上下文
- 为了最小化损失，它们的向量必须相似！

结果:
vec_编程 ≈ vec_Python ≈ [0.8, 0.2, -0.5, ...]
```

---

## 🧮 数学原理（为什么会收敛到相似向量）

### 优化目标

```
目标: 最大化这个概率

P(context|center) = exp(vec_center · vec_context) / Σ exp(vec_center · vec_i)
                     ↑ 内积越大，概率越大

如果"编程"和"Python"经常出现在相同上下文:
- 它们需要和相同的中心词有高概率
- 这要求它们的向量相似（内积大）

例子:
中心词"学习" → 上下文词"编程"  (出现100次)
中心词"学习" → 上下文词"Python" (出现80次)

为了最大化概率:
vec_编程 和 vec_学习 内积要大 (100次训练)
vec_Python 和 vec_学习 内积要大 (80次训练)

结果 → vec_编程 ≈ vec_Python
```

---

## 🌟 训练方法2: GloVe（全局向量）

### 由Stanford在2014年发明

**核心思想：** 直接利用词共现统计

```
第1步: 统计共现矩阵

        我    爱    学习   编程   Python
我      0     50    30     10     5
爱      50    0     45     8      6
学习    30    45    0      80     75
编程    10    8     80     0      90
Python  5     6     75     90     0

意义: "学习"和"编程"共同出现80次
```

```
第2步: 矩阵分解

目标: 找到词向量，使得
vec_i · vec_j ≈ log(共现次数_ij)

例如:
vec_学习 · vec_编程 ≈ log(80) = 4.38
vec_学习 · vec_Python ≈ log(75) = 4.32
```

**优点：** 比Word2Vec更快，效果更好

---

## 🚀 现代方法: Transformer的词嵌入

### BERT、GPT等使用的方法

**核心区别：上下文相关的向量**

```
Word2Vec/GloVe: 
每个词只有一个固定向量

"银行" → [0.1, 0.2, 0.3, ...]  
(不管在什么句子里都一样)

问题:
句子1: "我去银行存钱"  ← 金融机构
句子2: "河流的银行很陡" ← 河岸
→ 同一个向量无法表示两种意思！
```

```
BERT/GPT: 
每个词的向量根据上下文动态生成

句子1: "我去银行存钱"
"银行"的向量 = [0.8, 0.1, 0.2, ...] ← 偏向金融

句子2: "河流的银行很陡"
"银行"的向量 = [0.2, 0.7, 0.3, ...] ← 偏向地理

→ 同一个词在不同上下文有不同向量！
```

### Transformer训练过程

```python
# 训练任务: 完形填空（Masked Language Model）

原句: "我爱学习[MASK]智能"
       ↓
模型: "我爱学习[人工]智能"  ← 预测被遮住的词

训练目标: 
根据上下文"我爱学习___智能"，预测中间是什么词

通过这种训练，模型学会:
1. 理解上下文
2. 每个词在不同上下文的表示
```

---

## 💡 完整训练流程总结

### Word2Vec训练流程

```
┌─────────────────────────────────────┐
│  第1步: 收集文本数据                 │
│  - 维基百科、新闻、书籍...           │
│  - 几十亿个句子                      │
└──────────────┬──────────────────────┘
               ↓
┌─────────────────────────────────────┐
│  第2步: 构建词表                     │
│  - 统计所有出现的词                  │
│  - 建立词→ID的映射                  │
│  - 词表大小: 50000-500000           │
└──────────────┬──────────────────────┘
               ↓
┌─────────────────────────────────────┐
│  第3步: 生成训练样本                 │
│  - Skip-gram: 中心词→上下文         │
│  - CBOW: 上下文→中心词              │
│  - 几百亿个训练对                    │
└──────────────┬──────────────────────┘
               ↓
┌─────────────────────────────────────┐
│  第4步: 初始化神经网络               │
│  - W1: vocab_size × embedding_dim   │
│  - W2: embedding_dim × vocab_size   │
│  - 随机初始化权重                    │
└──────────────┬──────────────────────┘
               ↓
┌─────────────────────────────────────┐
│  第5步: 训练（关键！）               │
│  For 每个训练对:                     │
│    1. 前向传播: 计算预测概率          │
│    2. 计算损失: 与真实标签比较        │
│    3. 反向传播: 计算梯度             │
│    4. 更新权重: 调整向量             │
│                                     │
│  重复几百万次迭代...                 │
└──────────────┬──────────────────────┘
               ↓
┌─────────────────────────────────────┐
│  第6步: 提取词向量                   │
│  - W1矩阵的每一行就是词向量          │
│  - 保存到文件供使用                  │
└─────────────────────────────────────┘
```

---

## 🔍 如何判断哪些词相似？（训练后）

### 方法1: 余弦相似度（最常用）

```python
def find_similar_words(word, word_vectors, vocab, top_k=5):
    """找到最相似的词"""
    if word not in vocab:
        return []
    
    word_vec = word_vectors[vocab[word]]
    similarities = []
    
    # 与词表中所有词计算相似度
    for other_word, idx in vocab.items():
        if other_word == word:
            continue
        
        other_vec = word_vectors[idx]
        sim = cosine_similarity(word_vec, other_vec)
        similarities.append((other_word, sim))
    
    # 排序返回最相似的top_k个
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# 使用示例
similar_to_python = find_similar_words("Python", word_vectors, vocab)
print(similar_to_python)
# [('Java', 0.85), ('JavaScript', 0.83), ('编程', 0.81), ...]
```

---

### 方法2: 向量算术（神奇的属性！）

```python
# 著名的例子: 国王 - 男人 + 女人 = 女王

vec_king = word_vectors[vocab["国王"]]
vec_man = word_vectors[vocab["男人"]]
vec_woman = word_vectors[vocab["女人"]]

# 计算
result_vec = vec_king - vec_man + vec_woman

# 找到最接近result_vec的词
similar = find_most_similar_vector(result_vec, word_vectors, vocab)
print(similar[0])  # "女王" (概率很高！)

# 更多例子:
# 东京 - 日本 + 法国 ≈ 巴黎
# 走 - 走路 + 游泳 ≈ 游
# 好 - 更好 + 大 ≈ 更大
```

**为什么可以做向量算术？**

```
训练让向量捕捉到了语义关系:

vec_国王 - vec_男人 = [性别:男] + [身份:君主]
vec_女人 - vec_男人 = [性别差异]

所以:
vec_国王 - vec_男人 + vec_女人
= [身份:君主] + [性别:女]
≈ vec_女王
```

---

## 🎯 实战：使用预训练的词向量

### 不需要自己训练！（除非特殊需求）

```python
# 方法1: 使用 Gensim 加载预训练模型
from gensim.models import KeyedVectors

# 下载Google的Word2Vec模型（3GB）
# https://code.google.com/archive/p/word2vec/
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)

# 使用
print(model['computer'])  # 向量
print(model.most_similar('computer', topn=5))  
# [('computers', 0.72), ('software', 0.68), ...]

# 向量算术
result = model.most_similar(positive=['king', 'woman'], negative=['man'])
print(result[0])  # ('queen', 0.71)
```

```python
# 方法2: 使用 HuggingFace Transformers（现代方法）
from transformers import AutoTokenizer, AutoModel
import torch

# 加载BERT模型
tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
model = AutoModel.from_pretrained('bert-base-chinese')

# 获取词向量
text = "我爱学习人工智能"
inputs = tokenizer(text, return_tensors='pt')
outputs = model(**inputs)

# outputs.last_hidden_state: (1, seq_len, 768)
# 每个字/词都有一个768维的向量
print(outputs.last_hidden_state.shape)
```

---

## 📊 训练词向量需要多少数据？

### 数据量对比

```
小规模（够用）:
- 数据: 1-10GB文本
- 训练时间: 1-2天（GPU）
- 词表: 50000词
- 质量: 基本可用

中等规模（较好）:
- 数据: 50-100GB文本
- 训练时间: 1-2周（多GPU）
- 词表: 200000词
- 质量: 较好

大规模（最好）:
- 数据: 500GB-1TB文本
- 训练时间: 数周到数月
- 词表: 500000+词
- 质量: 很好（Google/OpenAI的模型）
```

---

## 🎓 总结

### 词向量如何训练？

```
1. 收集海量文本 (几十亿句话)
2. 统计词的共现关系
3. 用神经网络学习
4. 经常一起出现的词 → 相似的向量
5. 训练几百万次后收敛
```

### 如何判断相似？

```
方法1: 余弦相似度
- 向量夹角越小 → 越相似

方法2: 向量算术
- 可以做加减法
- 捕捉语义关系
```

### 关键点

1. **不是人为规定** - 完全靠训练学习
2. **基于统计** - 上下文相似 → 向量相似
3. **自动学习语义** - 神经网络自己发现规律
4. **可以直接用** - 有很多预训练模型

---

## 🔗 相关文档

- `AI如何理解人类语言与RAG技术.md` - 了解整体流程
- `矩阵的作用.md` - 理解向量和矩阵
- `为什么矩阵更快.md` - 为什么要用向量表示

---

**记住：词向量是AI从几十亿句话中自己学会的，就像小孩通过大量阅读学会理解词语一样！** 📚🤖
