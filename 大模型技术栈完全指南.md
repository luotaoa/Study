# å¤§æ¨¡åž‹æŠ€æœ¯æ ˆå®Œå…¨æŒ‡å—
é¢è¯•å¸¸é—®ï¼
~transformerï¼Œå‡ ä¸ªå˜ä½“ 
~decode onlyï¼Œç±»gptæ¨¡åž‹
~deepseek qwen æ¨¡åž‹è¿­ä»£çš„æ”¹è¿›ç‚¹
~ç”¨llamafactoryæ¡†æž¶å¾®è°ƒlora å‡ ä¸ªæ¨¡åž‹
~æŽ¨ç†åŠ é€Ÿæ¡†æž¶çš„ä½¿ç”¨  vllm sglangï¼ŒåŽŸç†çœ‹çœ‹
ï½žnative rag ï¼Œæ‰‹æ“ä¸€ä¸‹ï¼Œlangchain llamaindexè¿™ä¸¤ä¸ªæ¡†æž¶ä¼˜å…ˆã€‚ç„¶åŽgraphragä¹Ÿè¦çœ‹çœ‹ï¼Œç®€åŽ†ä¸Šè¿™ä¸ªgraphragæˆ–è€…agentic rag
## ðŸ“š ç›®å½•

1. [Transformeræž¶æž„åŠå˜ä½“](#transformeræž¶æž„åŠå˜ä½“)
2. [Decoder-onlyæ¨¡åž‹ï¼ˆGPTç³»åˆ—ï¼‰](#decoder-onlyæ¨¡åž‹)
3. [DeepSeekä¸ŽQwenæ¨¡åž‹æ”¹è¿›ç‚¹](#deepseekä¸Žqwenæ¨¡åž‹æ”¹è¿›ç‚¹)
4. [LlamaFactoryæ¡†æž¶ä¸ŽLoRAå¾®è°ƒ](#llamafactoryæ¡†æž¶ä¸Žloraå¾®è°ƒ)
5. [æŽ¨ç†åŠ é€Ÿæ¡†æž¶](#æŽ¨ç†åŠ é€Ÿæ¡†æž¶)
6. [RAGæŠ€æœ¯æ ˆ](#ragæŠ€æœ¯æ ˆ)

---

## ðŸ—ï¸ Transformeræž¶æž„åŠå˜ä½“

### TransformeråŸºç¡€æž¶æž„

**å‘æ˜Ž**ï¼š2017å¹´ï¼ŒGoogleè®ºæ–‡ã€ŠAttention is All You Needã€‹

#### æ ¸å¿ƒç»“æž„

```
åŽŸå§‹Transformer = Encoder + Decoder

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Transformer                 â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Encoder  â”‚ ---> â”‚ Decoder  â”‚   â”‚
â”‚  â”‚          â”‚      â”‚          â”‚   â”‚
â”‚  â”‚ NÃ—Layer  â”‚      â”‚ NÃ—Layer  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚
â”‚  ç”¨äºŽï¼šæœºå™¨ç¿»è¯‘                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Encoderå±‚ç»“æž„

```python
# æ¯ä¸ªEncoderå±‚åŒ…å«ï¼š
class EncoderLayer:
    def __init__(self):
        self.self_attention = MultiHeadAttention()
        self.feed_forward = FeedForward()
        self.norm1 = LayerNorm()
        self.norm2 = LayerNorm()
    
    def forward(self, x):
        # 1. Self-Attention
        attn_output = self.self_attention(x, x, x)
        x = self.norm1(x + attn_output)  # æ®‹å·®è¿žæŽ¥
        
        # 2. Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm2(x + ff_output)  # æ®‹å·®è¿žæŽ¥
        
        return x
```

#### Decoderå±‚ç»“æž„

```python
class DecoderLayer:
    def __init__(self):
        self.self_attention = MultiHeadAttention()  # Masked
        self.cross_attention = MultiHeadAttention()
        self.feed_forward = FeedForward()
        self.norm1 = LayerNorm()
        self.norm2 = LayerNorm()
        self.norm3 = LayerNorm()
    
    def forward(self, x, encoder_output):
        # 1. Masked Self-Attentionï¼ˆåªèƒ½çœ‹åˆ°å‰é¢çš„è¯ï¼‰
        attn_output = self.self_attention(x, x, x, mask=True)
        x = self.norm1(x + attn_output)
        
        # 2. Cross-Attentionï¼ˆçœ‹Encoderçš„è¾“å‡ºï¼‰
        cross_attn = self.cross_attention(x, encoder_output, encoder_output)
        x = self.norm2(x + cross_attn)
        
        # 3. Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + ff_output)
        
        return x
```

---

### Transformerçš„ä¸‰å¤§å˜ä½“

```
Transformerå®¶æ—
â”‚
â”œâ”€ Encoder-onlyï¼ˆBERTç³»åˆ—ï¼‰
â”‚   â””â”€ é€‚åˆï¼šç†è§£ä»»åŠ¡
â”‚
â”œâ”€ Decoder-onlyï¼ˆGPTç³»åˆ—ï¼‰â­
â”‚   â””â”€ é€‚åˆï¼šç”Ÿæˆä»»åŠ¡
â”‚
â””â”€ Encoder-Decoderï¼ˆT5ã€BARTï¼‰
    â””â”€ é€‚åˆï¼šç¿»è¯‘ã€æ‘˜è¦
```

#### 1ï¸âƒ£ Encoder-onlyï¼ˆBERTç³»åˆ—ï¼‰

**ä»£è¡¨æ¨¡åž‹**ï¼šBERTã€RoBERTaã€ALBERT

```
æž¶æž„ï¼šåªè¦Encoderéƒ¨åˆ†

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input    â”‚
â”‚  Tokens    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Encoder 1  â”‚ â† åŒå‘æ³¨æ„åŠ›ï¼ˆèƒ½çœ‹åˆ°å‰åŽæ‰€æœ‰è¯ï¼‰
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Encoder 2  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    ...     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Encoder 12 â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output   â”‚
â”‚ Embeddings â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç‰¹ç‚¹ï¼š
âœ… åŒå‘ä¸Šä¸‹æ–‡
âœ… é€‚åˆç†è§£ä»»åŠ¡
âŒ ä¸æ“…é•¿ç”Ÿæˆ
```

**åº”ç”¨åœºæ™¯**ï¼š
```python
# æ–‡æœ¬åˆ†ç±»
input: "è¿™éƒ¨ç”µå½±å¤ªæ£’äº†"
output: æƒ…æ„Ÿ=æ­£é¢ï¼ˆ0.95ï¼‰

# å‘½åå®žä½“è¯†åˆ«
input: "å°æ˜Žåœ¨åŒ—äº¬å·¥ä½œ"
output: å°æ˜Ž=äººåï¼ŒåŒ—äº¬=åœ°å

# é—®ç­”ç³»ç»Ÿï¼ˆæŠ½å–å¼ï¼‰
context: "å°æ˜Žä»Šå¹´25å²"
question: "å°æ˜Žå¤šå¤§ï¼Ÿ"
output: "25å²"
```

#### 2ï¸âƒ£ Decoder-onlyï¼ˆGPTç³»åˆ—ï¼‰â­ å½“å‰ä¸»æµ

**ä»£è¡¨æ¨¡åž‹**ï¼šGPT-3ã€GPT-4ã€LLaMAã€Qwenã€DeepSeek

```
æž¶æž„ï¼šåªè¦Decoderéƒ¨åˆ†ï¼ˆåŽ»æŽ‰Cross-Attentionï¼‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input    â”‚
â”‚  Tokens    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder 1  â”‚ â† å•å‘æ³¨æ„åŠ›ï¼ˆåªèƒ½çœ‹åˆ°å‰é¢çš„è¯ï¼‰
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Decoder 2  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    ...     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Decoder 96 â”‚ (GPT-3æœ‰96å±‚)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç‰¹ç‚¹ï¼š
âœ… è‡ªå›žå½’ç”Ÿæˆ
âœ… é€‚åˆæ–‡æœ¬ç”Ÿæˆ
âœ… å¯ä»¥æ— é™ç”Ÿæˆ
âœ… è®­ç»ƒç®€å•é«˜æ•ˆ
```

**ä¸ºä»€ä¹ˆDecoder-onlyæˆä¸ºä¸»æµï¼Ÿ**

```
1. è®­ç»ƒæ•ˆçŽ‡é«˜
   â”œâ”€ åªéœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
   â”œâ”€ ä¸éœ€è¦Encoder-Decoderå¯¹é½
   â””â”€ å¯ä»¥ç”¨æµ·é‡æ— æ ‡æ³¨æ•°æ®

2. ç”Ÿæˆèƒ½åŠ›å¼º
   â”œâ”€ è‡ªå›žå½’ç”Ÿæˆæµç•…
   â”œâ”€ é•¿æ–‡æœ¬ç”Ÿæˆæ•ˆæžœå¥½
   â””â”€ Few-shotèƒ½åŠ›å¼º

3. ç»Ÿä¸€æž¶æž„
   â”œâ”€ ç†è§£å’Œç”Ÿæˆç”¨åŒä¸€æž¶æž„
   â””â”€ ç®€åŒ–æ¨¡åž‹è®¾è®¡

4. æ‰©å±•æ€§å¥½
   â”œâ”€ å‚æ•°è¶Šå¤§æ•ˆæžœè¶Šå¥½
   â””â”€ æ¶ŒçŽ°èƒ½åŠ›ï¼ˆEmergent Abilitiesï¼‰
```

**åº”ç”¨åœºæ™¯**ï¼š
```python
# æ–‡æœ¬ç”Ÿæˆ
prompt: "ä»Žå‰æœ‰åº§å±±ï¼Œ"
output: "å±±ä¸Šæœ‰åº§åº™ï¼Œåº™é‡Œæœ‰ä¸ªè€å’Œå°š..."

# å¯¹è¯
user: "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
assistant: "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."

# ä»£ç ç”Ÿæˆ
prompt: "å†™ä¸€ä¸ªPythonå†’æ³¡æŽ’åº"
output: "def bubble_sort(arr): ..."

# ç¿»è¯‘
prompt: "Translate to English: ä½ å¥½"
output: "Hello"
```

#### 3ï¸âƒ£ Encoder-Decoderï¼ˆT5ã€BARTï¼‰

**ä»£è¡¨æ¨¡åž‹**ï¼šT5ã€BARTã€mT5

```
æž¶æž„ï¼šä¿ç•™å®Œæ•´Transformer

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Encoder  â”‚ --> â”‚  Decoder   â”‚
â”‚  (åŒå‘)    â”‚     â”‚  (å•å‘)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç‰¹ç‚¹ï¼š
âœ… æœ€çµæ´»
âœ… é€‚åˆSeq2Seqä»»åŠ¡
âŒ è®­ç»ƒæˆæœ¬é«˜
```

**åº”ç”¨åœºæ™¯**ï¼š
- æœºå™¨ç¿»è¯‘
- æ–‡æœ¬æ‘˜è¦
- é—®ç­”ç”Ÿæˆ

---

### æž¶æž„å¯¹æ¯”æ€»ç»“

| æž¶æž„                | ä»£è¡¨æ¨¡åž‹  | æ³¨æ„åŠ›æ–¹å‘ | æœ€é€‚åˆ   | å½“å‰æµè¡Œåº¦ |
| ------------------- | --------- | ---------- | -------- | ---------- |
| **Encoder-only**    | BERT      | åŒå‘       | ç†è§£ä»»åŠ¡ | â­â­â­        |
| **Decoder-only**    | GPT/LLaMA | å•å‘       | ç”Ÿæˆä»»åŠ¡ | â­â­â­â­â­      |
| **Encoder-Decoder** | T5        | åŒå‘+å•å‘  | ç¿»è¯‘æ‘˜è¦ | â­â­         |

**ç»“è®º**ï¼šå½“å‰å¤§æ¨¡åž‹å‡ ä¹Žå…¨æ˜¯Decoder-onlyæž¶æž„ï¼

---

## ðŸš€ Decoder-onlyæ¨¡åž‹è¯¦è§£ï¼ˆç±»GPTæ¨¡åž‹ï¼‰

### GPTç³»åˆ—æ¼”è¿›

```
GPTå®¶æ—æ¼”è¿›å²

GPT-1 (2018)
â”œâ”€ å‚æ•°ï¼š117M
â”œâ”€ å±‚æ•°ï¼š12å±‚
â””â”€ è¯æ˜Žï¼šé¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼æœ‰æ•ˆ

GPT-2 (2019)
â”œâ”€ å‚æ•°ï¼š1.5B
â”œâ”€ å±‚æ•°ï¼š48å±‚
â””â”€ è¯æ˜Žï¼šzero-shotèƒ½åŠ›

GPT-3 (2020) â­
â”œâ”€ å‚æ•°ï¼š175B
â”œâ”€ å±‚æ•°ï¼š96å±‚
â”œâ”€ ä¸Šä¸‹æ–‡ï¼š2048 tokens
â””â”€ è¯æ˜Žï¼šfew-shot learning

GPT-3.5 (2022)
â”œâ”€ åŠ å…¥ï¼šRLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰
â””â”€ ChatGPTçˆ†ç«

GPT-4 (2023)
â”œâ”€ å‚æ•°ï¼šæœªå…¬å¼€ï¼ˆæ®è¯´1.7Tï¼‰
â”œâ”€ å¤šæ¨¡æ€ï¼ˆå›¾æ–‡ï¼‰
â”œâ”€ ä¸Šä¸‹æ–‡ï¼š128K tokens
â””â”€ æŽ¨ç†èƒ½åŠ›å¤§å¹…æå‡
```

### Decoder-onlyæ ¸å¿ƒæœºåˆ¶

#### 1. å› æžœæ³¨æ„åŠ›ï¼ˆCausal Attentionï¼‰

```python
# å…³é”®ï¼šåªèƒ½çœ‹åˆ°å‰é¢çš„è¯
def causal_attention(Q, K, V):
    scores = Q @ K.T  # è®¡ç®—ç›¸ä¼¼åº¦
    
    # ç”Ÿæˆå› æžœæŽ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
    mask = torch.tril(torch.ones(seq_len, seq_len))
    #      ä»Š  å¤©  å¤©  æ°”  å¾ˆ  å¥½
    # ä»Š   1   0   0   0   0   0
    # å¤©   1   1   0   0   0   0
    # å¤©   1   1   1   0   0   0
    # æ°”   1   1   1   1   0   0
    # å¾ˆ   1   1   1   1   1   0
    # å¥½   1   1   1   1   1   1
    
    scores = scores.masked_fill(mask == 0, -inf)
    attention = softmax(scores)
    output = attention @ V
    
    return output

# æ•ˆæžœï¼š
# "ä»Šå¤©" åªèƒ½çœ‹åˆ° "ä»Šå¤©"
# "å¤©æ°”" å¯ä»¥çœ‹åˆ° "ä»Šå¤©å¤©æ°”"
# "å¾ˆå¥½" å¯ä»¥çœ‹åˆ° "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
```

#### 2. è‡ªå›žå½’ç”Ÿæˆï¼ˆAutoregressive Generationï¼‰

```python
# ç”Ÿæˆè¿‡ç¨‹ï¼ˆä¸€ä¸ªè¯ä¸€ä¸ªè¯ç”Ÿæˆï¼‰

def generate(prompt, max_length=50):
    tokens = tokenize(prompt)  # "ä»Šå¤©å¤©æ°”"
    
    for i in range(max_length):
        # 1. è¾“å…¥å½“å‰æ‰€æœ‰token
        logits = model(tokens)  # [batch, seq_len, vocab_size]
        
        # 2. åªå–æœ€åŽä¸€ä¸ªtokençš„é¢„æµ‹
        next_token_logits = logits[:, -1, :]
        
        # 3. é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_token = sample(next_token_logits)  # "å¾ˆ"
        
        # 4. æ·»åŠ åˆ°åºåˆ—
        tokens = torch.cat([tokens, next_token])
        
        # 5. å¦‚æžœæ˜¯ç»“æŸç¬¦ï¼Œåœæ­¢
        if next_token == EOS:
            break
    
    return tokens

# ç”Ÿæˆç¤ºä¾‹ï¼š
# è¾“å…¥ï¼š   "ä»Šå¤©å¤©æ°”"
# ç¬¬1æ­¥ï¼š  "ä»Šå¤©å¤©æ°”å¾ˆ"
# ç¬¬2æ­¥ï¼š  "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
# ç¬¬3æ­¥ï¼š  "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ"
# ç¬¬4æ­¥ï¼š  "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆ"
# ...
```

#### 3. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

```python
# æ–¹å¼1ï¼šç»å¯¹ä½ç½®ç¼–ç ï¼ˆGPT-2ï¼‰
def absolute_position_encoding(seq_len, d_model):
    position = torch.arange(seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) * 
                         -(math.log(10000.0) / d_model))
    
    pe = torch.zeros(seq_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe

# æ–¹å¼2ï¼šç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆT5ï¼‰
# æ–¹å¼3ï¼šæ—‹è½¬ä½ç½®ç¼–ç  RoPEï¼ˆLLaMAï¼‰â­ å½“å‰ä¸»æµ
def rotary_position_embedding(x, position):
    # é€šè¿‡æ—‹è½¬çŸ©é˜µç¼–ç ä½ç½®ä¿¡æ¯
    # ä¼˜åŠ¿ï¼šå¤–æŽ¨èƒ½åŠ›å¼º
    return rotate(x, position)
```

---

### å½“å‰ä¸»æµDecoder-onlyæ¨¡åž‹

| æ¨¡åž‹            | å‚æ•°é‡   | ä¸Šä¸‹æ–‡ | ç‰¹ç‚¹           | å¼€æº |
| --------------- | -------- | ------ | -------------- | ---- |
| **GPT-4**       | 1.7T?    | 128K   | æœ€å¼ºï¼Œå¤šæ¨¡æ€   | âŒ    |
| **Claude 3**    | æœªçŸ¥     | 200K   | æŽ¨ç†å¼ºï¼Œé•¿æ–‡æœ¬ | âŒ    |
| **LLaMA 3**     | 8B-70B   | 8K     | å¼€æºæ ‡æ†       | âœ…    |
| **Qwen 2.5**    | 0.5B-72B | 128K   | ä¸­æ–‡å¼ºï¼Œå¤šæ¨¡æ€ | âœ…    |
| **DeepSeek V2** | 236B     | 128K   | MoEï¼Œä¾¿å®œ      | âœ…    |
| **GLM-4**       | 9B       | 128K   | ä¸­æ–‡å¯¹è¯       | âœ…    |

---

## ðŸ”¥ DeepSeekä¸ŽQwenæ¨¡åž‹æ”¹è¿›ç‚¹

### DeepSeekç³»åˆ—

#### DeepSeek V1 (2023)

**åŸºç¡€æž¶æž„**ï¼šæ ‡å‡†Decoder-only + ä¼˜åŒ–

```
æ”¹è¿›ç‚¹1ï¼šè®­ç»ƒæ•°æ®ä¼˜åŒ–
â”œâ”€ 2Té«˜è´¨é‡tokens
â”œâ”€ ä¸­è‹±åŒè¯­å¹³è¡¡
â””â”€ ä»£ç æ•°æ®å¢žå¼º

æ”¹è¿›ç‚¹2ï¼šè®­ç»ƒç­–ç•¥
â”œâ”€ å¤šé˜¶æ®µè®­ç»ƒ
â”œâ”€ è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰
â””â”€ é•¿çŸ­æ–‡æœ¬æ··åˆè®­ç»ƒ
```

#### DeepSeek V2 (2024) â­ é‡å¤§çªç ´

**æ ¸å¿ƒåˆ›æ–°**ï¼šMoEæž¶æž„ï¼ˆæ··åˆä¸“å®¶æ¨¡åž‹ï¼‰

```
MoE (Mixture of Experts)

ä¼ ç»Ÿæ¨¡åž‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ‰€æœ‰å‚æ•°    â”‚  â† æ¯æ¬¡æŽ¨ç†éƒ½ç”¨å…¨éƒ¨å‚æ•°
â”‚  (236B)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MoEæ¨¡åž‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å…±äº«å‚æ•° (21B)               â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  64ä¸ªä¸“å®¶ (æ¯ä¸ª3.5B)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”   â”‚
â”‚  â”‚ä¸“å®¶1â”‚â”‚ä¸“å®¶2â”‚â”‚ä¸“å®¶3â”‚â”‚ä¸“å®¶4â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜   â”‚
â”‚         ...                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“ è·¯ç”±å™¨é€‰æ‹©Top-Kä¸“å®¶
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¿€æ´»å‚æ•° (21B + 2Ã—3.5B)      â”‚ â† åªç”¨éƒ¨åˆ†ä¸“å®¶
â”‚  = 28B æ¿€æ´»å‚æ•°               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¼˜åŠ¿ï¼š
âœ… æ€»å‚æ•°ï¼š236B
âœ… æ¿€æ´»å‚æ•°ï¼š28Bï¼ˆåªæœ‰12%ï¼‰
âœ… æŽ¨ç†æˆæœ¬ï¼šç­‰åŒäºŽ28Bæ¨¡åž‹
âœ… æ•ˆæžœï¼šæŽ¥è¿‘236Bæ¨¡åž‹
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š

```python
class DeepSeekV2Layer:
    def __init__(self):
        self.shared_ffn = FeedForward()
        self.experts = [Expert() for _ in range(64)]
        self.router = Router()
    
    def forward(self, x):
        # 1. å…±äº«FFN
        shared_output = self.shared_ffn(x)
        
        # 2. è·¯ç”±å™¨é€‰æ‹©ä¸“å®¶
        expert_scores = self.router(x)  # [batch, 64]
        top2_experts = torch.topk(expert_scores, k=2)
        
        # 3. åªæ¿€æ´»Top-2ä¸“å®¶
        expert_output = 0
        for expert_id, weight in top2_experts:
            expert_output += weight * self.experts[expert_id](x)
        
        # 4. ç»„åˆ
        output = shared_output + expert_output
        
        return output
```

**æ”¹è¿›ç‚¹æ€»ç»“**ï¼š

```
DeepSeek V2æ ¸å¿ƒæ”¹è¿›
â”œâ”€ MoEæž¶æž„
â”‚   â”œâ”€ 64ä¸“å®¶
â”‚   â”œâ”€ Top-2è·¯ç”±
â”‚   â””â”€ æˆæœ¬é™ä½Ž95%
â”‚
â”œâ”€ MLAï¼ˆMulti-head Latent Attentionï¼‰
â”‚   â”œâ”€ åŽ‹ç¼©KV Cache
â”‚   â””â”€ é•¿æ–‡æœ¬æŽ¨ç†æ›´å¿«
â”‚
â”œâ”€ 128Kä¸Šä¸‹æ–‡
â”‚   â””â”€ YaRNä½ç½®ç¼–ç 
â”‚
â””â”€ è®­ç»ƒä¼˜åŒ–
    â”œâ”€ FP8æ··åˆç²¾åº¦
    â””â”€ Pipelineå¹¶è¡Œ
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| æŒ‡æ ‡     | DeepSeek V2    | GPT-4 | Claude 3 |
| -------- | -------------- | ----- | -------- |
| **å‚æ•°** | 236B (æ¿€æ´»21B) | 1.7T? | æœªçŸ¥     |
| **æˆæœ¬** | $0.14/M tokens | $30/M | $15/M    |
| **é€Ÿåº¦** | æžå¿«           | å¿«    | å¿«       |
| **æ•ˆæžœ** | â­â­â­â­           | â­â­â­â­â­ | â­â­â­â­â­    |

---

### Qwenç³»åˆ—ï¼ˆé€šä¹‰åƒé—®ï¼‰

#### Qwen 1.0 (2023)

```
åŸºç¡€æ¨¡åž‹ï¼š
â”œâ”€ å‚æ•°ï¼š0.5B - 72B
â”œâ”€ ä¸Šä¸‹æ–‡ï¼š8K - 32K
â””â”€ å¤šæ¨¡æ€ï¼šæ–‡æœ¬+å›¾åƒ
```

#### Qwen 1.5 (2024.2)

**æ”¹è¿›ç‚¹**ï¼š

```
1. æ›´é•¿ä¸Šä¸‹æ–‡
   â”œâ”€ ä»Ž32K â†’ 128K
   â””â”€ YaRNä½ç½®ç¼–ç 

2. æ›´å¥½çš„æŒ‡ä»¤éµå¾ª
   â”œâ”€ æ”¹è¿›SFTæ•°æ®
   â””â”€ æ›´å¼ºçš„RLHF

3. ä»£ç èƒ½åŠ›æå‡
   â””â”€ ä»£ç æ•°æ®å¢žå¼º
```

#### Qwen 2.0 (2024.6)

```
æ”¹è¿›ç‚¹1ï¼šæ¨¡åž‹æž¶æž„ä¼˜åŒ–
â”œâ”€ GQAï¼ˆGrouped Query Attentionï¼‰
â”‚   â”œâ”€ å‡å°‘KV Cache
â”‚   â””â”€ æŽ¨ç†é€Ÿåº¦æå‡30%
â”‚
â””â”€ SwiGLUæ¿€æ´»å‡½æ•°
    â””â”€ æ›¿ä»£ReLUï¼Œæ•ˆæžœæ›´å¥½

æ”¹è¿›ç‚¹2ï¼šè®­ç»ƒæ•°æ®ä¼˜åŒ–
â”œâ”€ 7Té«˜è´¨é‡tokensï¼ˆ2å€äºŽQwen1ï¼‰
â”œâ”€ å¤šè¯­è¨€æ•°æ®å¢žå¼ºï¼ˆ29ç§è¯­è¨€ï¼‰
â””â”€ ä»£ç æ•°æ®æ¯”ä¾‹æå‡

æ”¹è¿›ç‚¹3ï¼šé•¿æ–‡æœ¬èƒ½åŠ›
â”œâ”€ åŽŸç”Ÿæ”¯æŒ128Kä¸Šä¸‹æ–‡
â”œâ”€ Sparse Attention
â””â”€ é•¿æ–‡æœ¬æ€§èƒ½ä¸è¡°å‡
```

**GQAè¯¦è§£**ï¼š

```python
# Multi-Head Attention (åŽŸå§‹)
class MHA:
    def __init__(self, d_model=4096, n_heads=32):
        self.n_heads = 32
        self.n_kv_heads = 32  # KVä¹Ÿæ˜¯32ä¸ªå¤´
        
        self.wq = Linear(d_model, d_model)
        self.wk = Linear(d_model, d_model)  # 32ä¸ªå¤´
        self.wv = Linear(d_model, d_model)  # 32ä¸ªå¤´
    
    # KV Cacheå¤§å°ï¼š[batch, 32, seq_len, head_dim]

# Grouped Query Attention (Qwen 2)
class GQA:
    def __init__(self, d_model=4096, n_heads=32):
        self.n_heads = 32
        self.n_kv_heads = 4  # â­ KVåªæœ‰4ä¸ªå¤´
        
        self.wq = Linear(d_model, d_model)
        self.wk = Linear(d_model, d_model // 8)  # 4ä¸ªå¤´
        self.wv = Linear(d_model, d_model // 8)  # 4ä¸ªå¤´
    
    # KV Cacheå¤§å°ï¼š[batch, 4, seq_len, head_dim]
    # èŠ‚çœå†…å­˜ï¼š75%ï¼

# å·¥ä½œåŽŸç†ï¼š
# 32ä¸ªQå¤´ â†’ åˆ†æˆ8ç»„
# æ¯ç»„4ä¸ªQå¤´ â†’ å…±äº«1ä¸ªKVå¤´
#
# Q1, Q2, Q3, Q4  â†’ å…±äº« KV1
# Q5, Q6, Q7, Q8  â†’ å…±äº« KV2
# ...
```

#### Qwen 2.5 (2024.9) â­ æœ€æ–°

**é‡å¤§æ”¹è¿›**ï¼š

```
æ”¹è¿›ç‚¹1ï¼šå‚æ•°èŒƒå›´æ‰©å¤§
â”œâ”€ æ–°å¢ž0.5Bã€3Bå°æ¨¡åž‹
â”œâ”€ 14Bã€32Bä¸­ç­‰æ¨¡åž‹
â””â”€ 72Bå¤§æ¨¡åž‹

æ”¹è¿›ç‚¹2ï¼šä¸“ä¸šèƒ½åŠ›æå‡
â”œâ”€ æ•°å­¦æŽ¨ç†ï¼ˆæå‡30%ï¼‰
â”œâ”€ ä»£ç ç”Ÿæˆï¼ˆæå‡40%ï¼‰
â””â”€ é•¿æ–‡æœ¬ç†è§£

æ”¹è¿›ç‚¹3ï¼šå¤šæ¨¡æ€å¢žå¼º
â”œâ”€ Qwen2-VLï¼ˆè§†è§‰ï¼‰
â”œâ”€ Qwen2-Audioï¼ˆè¯­éŸ³ï¼‰
â””â”€ ç»Ÿä¸€å¤šæ¨¡æ€æž¶æž„

æ”¹è¿›ç‚¹4ï¼šå·¥å…·ä½¿ç”¨
â”œâ”€ Function Call
â”œâ”€ Agentèƒ½åŠ›
â””â”€ å·¥å…·é“¾æ•´åˆ
```

**Qwen 2.5æž¶æž„å›¾**ï¼š

```
Qwen 2.5 å®Œæ•´æž¶æž„

è¾“å…¥æ–‡æœ¬
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenizer                   â”‚
â”‚ â”œâ”€ è¯è¡¨ï¼š151,936            â”‚
â”‚ â””â”€ BPEç¼–ç                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Layer             â”‚
â”‚ â”œâ”€ Token Embedding          â”‚
â”‚ â””â”€ RoPE Position Encoding   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layers (80å±‚)   â”‚
â”‚                              â”‚
â”‚ æ¯å±‚åŒ…å«ï¼š                   â”‚
â”‚ â”œâ”€ RMSNorm                  â”‚
â”‚ â”œâ”€ GQA (32å¤´Q + 4å¤´KV)      â”‚
â”‚ â”œâ”€ RMSNorm                  â”‚
â”‚ â””â”€ SwiGLU FFN               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer                â”‚
â”‚ â””â”€ LM Head (é¢„æµ‹ä¸‹ä¸€ä¸ªè¯)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç‰¹ç‚¹ï¼š
âœ… ä¸Šä¸‹æ–‡ï¼š128K
âœ… å‚æ•°ï¼š72B (7Bç‰ˆæœ¬ä¹Ÿå¾ˆå¼º)
âœ… é€Ÿåº¦ï¼šGQAåŠ é€Ÿ
âœ… ä¸­æ–‡ï¼šä¸–ç•Œæœ€å¼º
```

---

### DeepSeek vs Qwen å¯¹æ¯”

| ç»´åº¦       | DeepSeek V2    | Qwen 2.5     |
| ---------- | -------------- | ------------ |
| **æž¶æž„**   | MoE (64ä¸“å®¶)   | Dense (ä¼ ç»Ÿ) |
| **å‚æ•°**   | 236B (æ¿€æ´»21B) | 0.5B-72B     |
| **æˆæœ¬**   | æžä½Ž â­â­â­â­â­     | ä¸­ç­‰         |
| **é€Ÿåº¦**   | æžå¿« â­â­â­â­â­     | å¿« â­â­â­â­      |
| **ä¸­æ–‡**   | â­â­â­â­           | â­â­â­â­â­        |
| **æ•°å­¦**   | â­â­â­â­â­          | â­â­â­â­         |
| **ä»£ç **   | â­â­â­â­â­          | â­â­â­â­â­        |
| **å¤šæ¨¡æ€** | âŒ              | âœ…            |
| **å¼€æº**   | âœ…              | âœ…            |

**é€‰æ‹©å»ºè®®**ï¼š
- **è¿½æ±‚æ€§ä»·æ¯”** â†’ DeepSeek V2
- **è¿½æ±‚æ˜“ç”¨æ€§** â†’ Qwen 2.5
- **å°æ¨¡åž‹éƒ¨ç½²** â†’ Qwen 2.5 (0.5B-7B)
- **å¤§è§„æ¨¡æŽ¨ç†** â†’ DeepSeek V2 (æˆæœ¬ä½Ž)

---

## ðŸ› ï¸ LlamaFactoryæ¡†æž¶ä¸ŽLoRAå¾®è°ƒ

### LlamaFactoryç®€ä»‹

**å®šä½**ï¼šå¼€ç®±å³ç”¨çš„å¤§æ¨¡åž‹å¾®è°ƒæ¡†æž¶

```
LlamaFactory = ä¸€ç«™å¼LLMå¾®è°ƒå·¥å…·

æ”¯æŒï¼š
â”œâ”€ 100+ä¸»æµæ¨¡åž‹
â”‚   â”œâ”€ LLaMAç³»åˆ—
â”‚   â”œâ”€ Qwenç³»åˆ—
â”‚   â”œâ”€ ChatGLMç³»åˆ—
â”‚   â””â”€ ...
â”‚
â”œâ”€ å¤šç§å¾®è°ƒæ–¹æ³•
â”‚   â”œâ”€ LoRA â­
â”‚   â”œâ”€ QLoRA
â”‚   â”œâ”€ Full Fine-tuning
â”‚   â””â”€ Freeze
â”‚
â”œâ”€ å¯è§†åŒ–ç•Œé¢
â”‚   â””â”€ Gradio Web UI
â”‚
â””â”€ æ˜“ç”¨æ€§å¼º
    â””â”€ é…ç½®æ–‡ä»¶å³å¯
```

### å®‰è£…ä¸Žå¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. å®‰è£…ä¾èµ–
pip install -e .

# 3. å¯åŠ¨Web UI
llamafactory-cli webui
```

### LoRAå¾®è°ƒåŽŸç†

**é—®é¢˜**ï¼šå…¨é‡å¾®è°ƒå¤ªè´µï¼

```
å…¨é‡å¾®è°ƒï¼ˆFull Fine-tuningï¼‰

æ¨¡åž‹ï¼šQwen-7B (7Bå‚æ•°)
éœ€è¦æ˜¾å­˜ï¼š
â”œâ”€ æ¨¡åž‹å‚æ•°ï¼š7B Ã— 2 bytes = 14GB
â”œâ”€ æ¢¯åº¦ï¼š7B Ã— 2 bytes = 14GB
â”œâ”€ ä¼˜åŒ–å™¨çŠ¶æ€ï¼š7B Ã— 8 bytes = 56GB
â””â”€ æ¿€æ´»å€¼ï¼šçº¦30GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡ï¼š~114GB âŒ å¤ªè´µäº†ï¼

è€Œä¸”ï¼š
âŒ éœ€è¦A100/H100
âŒ è®­ç»ƒæ…¢
âŒ æ¯ä¸ªä»»åŠ¡éƒ½è¦ä¿å­˜æ•´ä¸ªæ¨¡åž‹
```

**LoRAè§£å†³æ–¹æ¡ˆ**ï¼šåªè®­ç»ƒæžå°‘å‚æ•°ï¼

```
LoRA (Low-Rank Adaptation)

æ ¸å¿ƒæ€æƒ³ï¼šå†»ç»“åŽŸæ¨¡åž‹ï¼Œåªè®­ç»ƒå°çŸ©é˜µ

åŽŸå§‹æƒé‡çŸ©é˜µï¼š
W âˆˆ R^(dÃ—k)  (ä¾‹å¦‚ï¼š4096Ã—4096)

LoRAåˆ†è§£ï¼š
W' = W + Î”W
Î”W = B Ã— A

å…¶ä¸­ï¼š
A âˆˆ R^(dÃ—r)  (r=8ï¼Œè¿œå°äºŽd)
B âˆˆ R^(rÃ—k)

å‚æ•°é‡å¯¹æ¯”ï¼š
åŽŸå§‹ï¼šd Ã— k = 4096 Ã— 4096 = 16Må‚æ•°
LoRAï¼šdÃ—r + rÃ—k = 4096Ã—8 + 8Ã—4096 = 65Kå‚æ•°
èŠ‚çœï¼š99.6%ï¼
```

**å¯è§†åŒ–ç†è§£**ï¼š

```
åŽŸå§‹Linearå±‚ï¼š
Input(4096ç»´) â†’ [W: 4096Ã—4096] â†’ Output(4096ç»´)

LoRAæ”¹é€ ï¼š
Input(4096ç»´) 
    â†“
    â”œâ†’ [W: 4096Ã—4096] (å†»ç»“â„ï¸) 
    â”‚      â†“
    â”‚    Output1
    â”‚      â†“
    â””â†’ [A: 4096Ã—8] â†’ [B: 8Ã—4096]
           â†“             â†“
         Output2 (å¯è®­ç»ƒðŸ”¥)
           â†“
      Output1 + Î±Ã—Output2
           â†“
      Final Output

åªè®­ç»ƒAå’ŒBï¼
```

**å®žçŽ°ä»£ç **ï¼š

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, original_layer, rank=8, alpha=16):
        super().__init__()
        self.original_layer = original_layer
        
        # å†»ç»“åŽŸå§‹æƒé‡
        for param in self.original_layer.parameters():
            param.requires_grad = False
        
        # LoRAçŸ©é˜µAå’ŒB
        d_in = original_layer.in_features
        d_out = original_layer.out_features
        
        self.lora_A = nn.Parameter(torch.randn(d_in, rank) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(rank, d_out))
        
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
    
    def forward(self, x):
        # åŽŸå§‹è¾“å‡ºï¼ˆå†»ç»“ï¼‰
        original_output = self.original_layer(x)
        
        # LoRAè¾“å‡º
        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling
        
        # ç›¸åŠ 
        return original_output + lora_output

# ä½¿ç”¨ç¤ºä¾‹
original_linear = nn.Linear(4096, 4096)
lora_linear = LoRALayer(original_linear, rank=8)

# å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in lora_linear.parameters() 
                      if p.requires_grad)
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")  # 65,536

total_params = sum(p.numel() for p in lora_linear.parameters())
print(f"æ€»å‚æ•°: {total_params:,}")  # 16,842,752

print(f"è®­ç»ƒå‚æ•°å æ¯”: {trainable_params/total_params*100:.2f}%")  # 0.39%
```

---

### LlamaFactoryå¾®è°ƒå®žæˆ˜

#### å‡†å¤‡æ•°æ®

```json
# data/my_data.json
[
    {
        "instruction": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹",
        "input": "å¦‚ä½•é€€è´§ï¼Ÿ",
        "output": "æ‚¨å¥½ï¼é€€è´§æµç¨‹å¦‚ä¸‹ï¼š\n1. ç™»å½•è´¦æˆ·\n2. è¿›å…¥è®¢å•é¡µé¢\n3. é€‰æ‹©è¦é€€è´§çš„å•†å“\n4. å¡«å†™é€€è´§åŽŸå› \n5. æäº¤ç”³è¯·"
    },
    {
        "instruction": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹",
        "input": "å‘è´§å¤šä¹…ï¼Ÿ",
        "output": "ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šåœ¨24å°æ—¶å†…å‘è´§ï¼Œåè¿œåœ°åŒºå¯èƒ½éœ€è¦3-5ä¸ªå·¥ä½œæ—¥é€è¾¾ã€‚"
    }
    // ... æ›´å¤šæ•°æ®
]
```

#### é…ç½®å¾®è°ƒå‚æ•°

```yaml
# examples/train_lora/qwen2_lora.yaml

### æ¨¡åž‹é…ç½®
model_name_or_path: Qwen/Qwen2-7B  # æ¨¡åž‹è·¯å¾„
stage: sft  # ç›‘ç£å¾®è°ƒ

### æ•°æ®é…ç½®
dataset: my_data  # æ•°æ®é›†åç§°
template: qwen  # Promptæ¨¡æ¿
cutoff_len: 2048  # æœ€å¤§é•¿åº¦

### LoRAé…ç½®
finetuning_type: lora
lora_target: all  # å¯¹æ‰€æœ‰linearå±‚åŠ LoRA
lora_rank: 8  # rank=8
lora_alpha: 16  # alpha=16

### è®­ç»ƒé…ç½®
output_dir: ./output/qwen2_lora
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 5e-5
lr_scheduler_type: cosine

### ä¼˜åŒ–é…ç½®
fp16: true  # æ··åˆç²¾åº¦
logging_steps: 10
save_steps: 500
```

#### è¿è¡Œå¾®è°ƒ

```bash
# æ–¹æ³•1ï¼šå‘½ä»¤è¡Œ
llamafactory-cli train examples/train_lora/qwen2_lora.yaml

# æ–¹æ³•2ï¼šWeb UI
llamafactory-cli webui
# åœ¨ç•Œé¢ä¸­é€‰æ‹©æ¨¡åž‹ã€æ•°æ®ã€å‚æ•°ï¼Œç‚¹å‡»"å¼€å§‹è®­ç»ƒ"

# æ–¹æ³•3ï¼šPythonè„šæœ¬
python src/train_bash.py \
    --model_name_or_path Qwen/Qwen2-7B \
    --do_train \
    --dataset my_data \
    --finetuning_type lora \
    --lora_rank 8 \
    --output_dir ./output/qwen2_lora \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --learning_rate 5e-5
```

#### è®­ç»ƒæ—¥å¿—

```
Loading model from Qwen/Qwen2-7B...
âœ… Model loaded (7.04B parameters)

Adding LoRA adapters...
âœ… LoRA adapters added (rank=8)
ðŸ“Š Trainable params: 4,718,592 (0.067%)
ðŸ“Š All params: 7,044,718,592

Loading dataset my_data...
âœ… Dataset loaded: 1000 samples

Starting training...
Epoch 1/3
Step 100/1500 | Loss: 1.234 | LR: 4.5e-5
Step 200/1500 | Loss: 0.876 | LR: 4.0e-5
...

Training completed!
âœ… Model saved to ./output/qwen2_lora
```

---

### QLoRAï¼šæ›´çœæ˜¾å­˜

**QLoRA = LoRA + é‡åŒ–**

```
QLoRAæ”¹è¿›ï¼š
1. 4bité‡åŒ–åŸºåº§æ¨¡åž‹
   â”œâ”€ FP16: 7B Ã— 2 bytes = 14GB
   â””â”€ 4bit: 7B Ã— 0.5 bytes = 3.5GB â­

2. åŒé‡åŒ–ï¼ˆDouble Quantizationï¼‰
   â””â”€ è¿žé‡åŒ–å‚æ•°ä¹Ÿé‡åŒ–

3. åˆ†é¡µä¼˜åŒ–å™¨
   â””â”€ CPU/GPUå†…å­˜äº¤æ¢

ç»“æžœï¼š
âœ… 6GBæ˜¾å­˜å¯å¾®è°ƒ7Bæ¨¡åž‹ï¼
âœ… 24GBæ˜¾å­˜å¯å¾®è°ƒ33Bæ¨¡åž‹ï¼
```

```python
# QLoRAå¾®è°ƒç¤ºä¾‹
llamafactory-cli train \
    --model_name_or_path Qwen/Qwen2-7B \
    --quantization_bit 4 \  # 4bité‡åŒ–â­
    --finetuning_type lora \
    --lora_rank 8 \
    --dataset my_data \
    --output_dir ./output/qwen2_qlora
```

---

### å¾®è°ƒæ–¹æ³•å¯¹æ¯”

| æ–¹æ³•                 | è®­ç»ƒå‚æ•° | æ˜¾å­˜éœ€æ±‚      | æ•ˆæžœ | é€‚ç”¨åœºæ™¯   |
| -------------------- | -------- | ------------- | ---- | ---------- |
| **Full Fine-tuning** | 100%     | æžé«˜(>80GB)   | æœ€å¥½ | æœ‰è¶…å¼ºç®—åŠ› |
| **LoRA**             | <1%      | ä¸­ç­‰(16-32GB) | å¥½   | ä¸€èˆ¬å¾®è°ƒâ­  |
| **QLoRA**            | <1%      | ä½Ž(6-12GB)    | å¥½   | æ˜¾å­˜å—é™â­â­ |
| **Freeze**           | 10-30%   | é«˜(40-60GB)   | è¾ƒå¥½ | ç‰¹å®šåœºæ™¯   |

---

### LlamaFactoryæ”¯æŒçš„æ¨¡åž‹

```
âœ… LLaMAç³»åˆ—ï¼šLLaMA, LLaMA-2, LLaMA-3
âœ… Qwenç³»åˆ—ï¼šQwen, Qwen1.5, Qwen2, Qwen2.5
âœ… ChatGLMç³»åˆ—ï¼šChatGLM2, ChatGLM3, GLM-4
âœ… Baichuanç³»åˆ—ï¼šBaichuan, Baichuan2
âœ… InternLMç³»åˆ—ï¼šInternLM, InternLM2
âœ… DeepSeekç³»åˆ—ï¼šDeepSeek, DeepSeek-Coder
âœ… Mistralç³»åˆ—ï¼šMistral, Mixtral (MoE)
âœ… Yiç³»åˆ—ï¼šYi, Yi-1.5
... 100+ models
```

---

## âš¡ æŽ¨ç†åŠ é€Ÿæ¡†æž¶

### ä¸ºä»€ä¹ˆéœ€è¦æŽ¨ç†åŠ é€Ÿï¼Ÿ

**é—®é¢˜**ï¼šåŽŸç”ŸPyTorchæŽ¨ç†å¤ªæ…¢ï¼

```
åŽŸç”ŸæŽ¨ç†ï¼ˆHugging Face Transformersï¼‰

Qwen-7Bæ¨¡åž‹
è¾“å…¥ï¼š100 tokens
ç”Ÿæˆï¼š100 tokens

ä½¿ç”¨å•å¡A100:
â”œâ”€ åžåé‡ï¼š~10 tokens/s
â”œâ”€ å»¶è¿Ÿï¼š10ç§’ç”Ÿæˆ100 tokens
â””â”€ GPUåˆ©ç”¨çŽ‡ï¼š30-40%

é—®é¢˜ï¼š
âŒ æ˜¾å­˜åˆ©ç”¨çŽ‡ä½Žï¼ˆå¤§é‡æµªè´¹ï¼‰
âŒ è®¡ç®—åˆ©ç”¨çŽ‡ä½Žï¼ˆGPUç©ºé—²å¤šï¼‰
âŒ ä¸æ”¯æŒæ‰¹å¤„ç†ä¼˜åŒ–
âŒ KV Cacheç®¡ç†ä½Žæ•ˆ
```

**è§£å†³æ–¹æ¡ˆ**ï¼šä¸“é—¨çš„æŽ¨ç†åŠ é€Ÿæ¡†æž¶

```
æŽ¨ç†åŠ é€Ÿæ¡†æž¶å¯¹æ¯”

åŽŸç”ŸPyTorch:    10 tokens/s
â†“ ä¼˜åŒ–åŽ
vLLM:          100-150 tokens/s (10-15å€æå‡â­)
SGLang:        120-180 tokens/s (12-18å€æå‡â­â­)
TensorRT-LLM:   200+ tokens/s (20å€æå‡)
```

---

### vLLMè¯¦è§£

**å®šä½**ï¼šç›®å‰æœ€æµè¡Œçš„å¼€æºæŽ¨ç†åŠ é€Ÿæ¡†æž¶

#### æ ¸å¿ƒåˆ›æ–°ï¼šPagedAttention

```
é—®é¢˜ï¼šKV Cacheæ˜¾å­˜ç®¡ç†ä½Žæ•ˆ

ä¼ ç»Ÿæ–¹å¼ï¼š
æ¯ä¸ªè¯·æ±‚é¢„åˆ†é…å›ºå®šå¤§å°KV Cache

è¯·æ±‚1: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ________] â† åªç”¨äº†60%ï¼Œæµªè´¹40%
è¯·æ±‚2: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ______________] â† åªç”¨äº†30%ï¼Œæµªè´¹70%
è¯·æ±‚3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ_____] â† åªç”¨äº†75%ï¼Œæµªè´¹25%

ç»“æžœï¼š
âŒ æ˜¾å­˜æµªè´¹ä¸¥é‡ï¼ˆå¹³å‡æµªè´¹50%ï¼‰
âŒ æ‰¹å¤„ç†å¤§å°å—é™
âŒ åžåé‡ä½Ž
```

**PagedAttentionè§£å†³æ–¹æ¡ˆ**ï¼š

```
çµæ„Ÿæ¥è‡ªæ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜åˆ†é¡µ

è™šæ‹Ÿå†…å­˜ â†’ ç‰©ç†å†…å­˜æ˜ å°„

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è™šæ‹Ÿå†…å­˜ï¼ˆè¿žç»­ï¼‰     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Page 0 â†’ ç‰©ç†å— 5   â”‚
â”‚ Page 1 â†’ ç‰©ç†å— 2   â”‚
â”‚ Page 2 â†’ ç‰©ç†å— 7   â”‚
â”‚ Page 3 â†’ ç‰©ç†å— 1   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç‰©ç†å†…å­˜ï¼ˆéžè¿žç»­ï¼‰   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [1][2][3][4][5]...  â”‚ â† çµæ´»åˆ†é…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KV Cacheä¹Ÿè¿™æ ·ç®¡ç†ï¼š
è¯·æ±‚1: Page 0,1,2    â†’ ç‰©ç†å— 1,5,8
è¯·æ±‚2: Page 0,1      â†’ ç‰©ç†å— 2,6
è¯·æ±‚3: Page 0,1,2,3  â†’ ç‰©ç†å— 3,4,7,9

ä¼˜åŠ¿ï¼š
âœ… æ˜¾å­˜åˆ©ç”¨çŽ‡ï¼š95%+ï¼ˆæŽ¥è¿‘ç†è®ºä¸Šé™ï¼‰
âœ… æ‰¹å¤„ç†å¤§å°ï¼šæå‡10å€
âœ… åžåé‡ï¼šæå‡20å€
```

**PagedAttentionåŽŸç†å›¾**ï¼š

```
Tokenç”Ÿæˆè¿‡ç¨‹

Step 1: ç”Ÿæˆç¬¬1ä¸ªtoken
â”Œâ”€â”€â”€â”€â”
â”‚Pageâ”‚  KV Cacheåˆ†é…1ä¸ªPage
â””â”€â”€â”€â”€â”˜

Step 2: ç”Ÿæˆç¬¬2-64ä¸ªtoken
â”Œâ”€â”€â”€â”€â”
â”‚Pageâ”‚  ç»§ç»­ç”¨åŒä¸€ä¸ªPageï¼ˆ64 tokens/pageï¼‰
â””â”€â”€â”€â”€â”˜

Step 3: ç”Ÿæˆç¬¬65ä¸ªtoken
â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”
â”‚Pageâ”‚â”‚Pageâ”‚  åˆ†é…ç¬¬2ä¸ªPage
â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜

çµæ´»ç®¡ç†ï¼š
âœ… æŒ‰éœ€åˆ†é…
âœ… åŠ¨æ€å¢žé•¿
âœ… å…±äº«KV Cacheï¼ˆå¤šä¸ªè¯·æ±‚å…±äº«å‰ç¼€ï¼‰
```

#### vLLMå®‰è£…ä¸Žä½¿ç”¨

```bash
# å®‰è£…
pip install vllm

# Python API
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡åž‹
llm = LLM(model="Qwen/Qwen2-7B",
          tensor_parallel_size=1,  # GPUæ•°é‡
          max_model_len=4096)      # æœ€å¤§é•¿åº¦

# é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=100
)

# å•ä¸ªç”Ÿæˆ
prompts = ["è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "]
outputs = llm.generate(prompts, sampling_params)
print(outputs[0].outputs[0].text)

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯ç¥žç»ç½‘ç»œï¼Ÿ",
    "ä»€ä¹ˆæ˜¯Transformerï¼Ÿ"
]
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    print(output.outputs[0].text)
```

#### vLLM OpenAI APIæœåŠ¡å™¨

```bash
# å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2-7B \
    --port 8000

# å®¢æˆ·ç«¯è°ƒç”¨
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2-7B",
        "prompt": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "max_tokens": 100
    }'

# æˆ–ç”¨OpenAI Pythonåº“
from openai import OpenAI
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # vLLMä¸éœ€è¦çœŸå®žkey
)

response = client.completions.create(
    model="Qwen/Qwen2-7B",
    prompt="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    max_tokens=100
)
print(response.choices[0].text)
```

#### vLLMæ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# 1. Tensorå¹¶è¡Œï¼ˆå¤šå¡æŽ¨ç†ï¼‰
llm = LLM(
    model="Qwen/Qwen2-72B",
    tensor_parallel_size=4  # ç”¨4å¼ GPUå¹¶è¡Œ
)

# 2. é‡åŒ–åŠ é€Ÿ
llm = LLM(
    model="Qwen/Qwen2-7B",
    quantization="awq",  # AWQ 4bité‡åŒ–
    # æˆ– quantization="gptq"
)

# 3. è°ƒæ•´batch size
llm = LLM(
    model="Qwen/Qwen2-7B",
    max_num_batched_tokens=8192,  # å¢žå¤§æ‰¹å¤„ç†
    max_num_seqs=256  # åŒæ—¶å¤„ç†çš„åºåˆ—æ•°
)

# 4. Flash Attention 2
llm = LLM(
    model="Qwen/Qwen2-7B",
    use_v2_block_manager=True,  # ä½¿ç”¨Flash Attention 2
)
```

---

### SGLangè¯¦è§£

**å®šä½**ï¼šæ–°ä¸€ä»£æŽ¨ç†æ¡†æž¶ï¼Œæ€§èƒ½è¶…è¶ŠvLLM

#### æ ¸å¿ƒåˆ›æ–°ï¼š

**1. RadixAttentionï¼ˆåŸºæ•°æ ‘ç¼“å­˜ï¼‰**

```
é—®é¢˜ï¼šå¤šä¸ªè¯·æ±‚ç»å¸¸æœ‰ç›¸åŒå‰ç¼€

ä¾‹å¦‚ï¼š
è¯·æ±‚1: "ä½ æ˜¯AIåŠ©æ‰‹ã€‚é—®é¢˜ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
è¯·æ±‚2: "ä½ æ˜¯AIåŠ©æ‰‹ã€‚é—®é¢˜ï¼šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
è¯·æ±‚3: "ä½ æ˜¯AIåŠ©æ‰‹ã€‚é—®é¢˜ï¼šä»€ä¹ˆæ˜¯ç¥žç»ç½‘ç»œï¼Ÿ"

å…±åŒå‰ç¼€ï¼š"ä½ æ˜¯AIåŠ©æ‰‹ã€‚é—®é¢˜ï¼š"

vLLMï¼šæ¯ä¸ªè¯·æ±‚å•ç‹¬è®¡ç®—å‰ç¼€ âŒ
SGLangï¼šå‰ç¼€å…±äº«ï¼Œåªè®¡ç®—ä¸€æ¬¡ âœ…
```

**RadixAttentionå®žçŽ°**ï¼š

```
åŸºæ•°æ ‘ç»“æž„ï¼ˆRadix Treeï¼‰

          [root]
            â†“
      "ä½ æ˜¯AIåŠ©æ‰‹ã€‚"
            â†“
        "é—®é¢˜ï¼š"
         â†™  â†“  â†˜
"ä»€ä¹ˆæ˜¯æœºå™¨" "ä»€ä¹ˆæ˜¯æ·±åº¦" "ä»€ä¹ˆæ˜¯ç¥žç»"
   â†“          â†“          â†“
 "å­¦ä¹ ï¼Ÿ"   "å­¦ä¹ ï¼Ÿ"   "ç½‘ç»œï¼Ÿ"

ä¼˜åŠ¿ï¼š
âœ… å‰ç¼€åªè®¡ç®—ä¸€æ¬¡
âœ… KV Cacheè‡ªåŠ¨å…±äº«
âœ… æ˜¾å­˜åˆ©ç”¨çŽ‡æ›´é«˜
âœ… é€‚åˆå¯¹è¯ã€Agentç­‰åœºæ™¯
```

**2. æ”¯æŒå¤æ‚æŽ§åˆ¶æµ**

```python
import sglang as sgl

@sgl.function
def multi_turn_conversation(s, user_input):
    # æ”¯æŒæ¡ä»¶åˆ¤æ–­
    s += "User: " + user_input
    s += "Assistant: "
    
    # ç”Ÿæˆå›žç­”
    s += sgl.gen("answer", max_tokens=100)
    
    # æ ¹æ®å›žç­”åˆ¤æ–­
    if "ä¸çŸ¥é“" in s["answer"]:
        s += "è®©æˆ‘æŸ¥ä¸€ä¸‹èµ„æ–™ã€‚"
        s += sgl.gen("search", tool="search")
    
    return s["answer"]
```

#### SGLangå®‰è£…ä¸Žä½¿ç”¨

```bash
# å®‰è£…
pip install "sglang[all]"

# Python API
import sglang as sgl

# å¯åŠ¨runtime
runtime = sgl.Runtime(
    model_path="Qwen/Qwen2-7B",
    tp_size=1  # Tensor parallel
)

# ç”Ÿæˆ
@sgl.function
def qa(s, question):
    s += "é—®é¢˜ï¼š" + question + "\nå›žç­”ï¼š"
    s += sgl.gen("answer", max_tokens=100)

# è¿è¡Œ
state = qa.run(question="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
print(state["answer"])
```

#### SGLang APIæœåŠ¡å™¨

```bash
# å¯åŠ¨æœåŠ¡å™¨
python -m sglang.launch_server \
    --model-path Qwen/Qwen2-7B \
    --port 8000

# å®¢æˆ·ç«¯è°ƒç”¨
import requests

response = requests.post(
    "http://localhost:8000/generate",
    json={
        "text": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "sampling_params": {
            "temperature": 0.7,
            "max_new_tokens": 100
        }
    }
)
print(response.json()["text"])
```

---

### vLLM vs SGLang å¯¹æ¯”

| ç»´åº¦               | vLLM           | SGLang         |
| ------------------ | -------------- | -------------- |
| **æ ¸å¿ƒæŠ€æœ¯**       | PagedAttention | RadixAttention |
| **KV Cacheç®¡ç†**   | åˆ†é¡µç®¡ç†       | åŸºæ•°æ ‘å…±äº«     |
| **æ€§èƒ½ï¼ˆå•è¯·æ±‚ï¼‰** | â­â­â­â­           | â­â­â­â­           |
| **æ€§èƒ½ï¼ˆæ‰¹é‡ï¼‰**   | â­â­â­â­â­          | â­â­â­â­â­          |
| **å‰ç¼€å…±äº«**       | âŒ              | âœ… â­            |
| **å¤æ‚æŽ§åˆ¶æµ**     | âŒ              | âœ… â­            |
| **æ˜“ç”¨æ€§**         | â­â­â­â­â­          | â­â­â­â­           |
| **ç”Ÿæ€**           | æˆç†Ÿ           | æ–°å…´           |
| **ç¨³å®šæ€§**         | â­â­â­â­â­          | â­â­â­â­           |

**é€‰æ‹©å»ºè®®**ï¼š
- **é€šç”¨æŽ¨ç†** â†’ vLLMï¼ˆæˆç†Ÿç¨³å®šï¼‰
- **å¤šè½®å¯¹è¯** â†’ SGLangï¼ˆå‰ç¼€å…±äº«ï¼‰
- **Agentåº”ç”¨** â†’ SGLangï¼ˆæŽ§åˆ¶æµå¼ºï¼‰
- **ç”Ÿäº§çŽ¯å¢ƒ** â†’ vLLMï¼ˆæ›´ç¨³å®šï¼‰

---

### å…¶ä»–æŽ¨ç†æ¡†æž¶

#### TensorRT-LLMï¼ˆNVIDIAï¼‰

```
æœ€å¿«çš„æŽ¨ç†æ¡†æž¶ï¼ˆéœ€è¦NVIDIA GPUï¼‰

ä¼˜åŠ¿ï¼š
âœ… æ€§èƒ½æœ€å¼ºï¼ˆæ¯”vLLMå¿«30-50%ï¼‰
âœ… NVIDIAä¼˜åŒ–
âœ… Flash Attention 2
âœ… é‡åŒ–æ”¯æŒå…¨é¢

åŠ£åŠ¿ï¼š
âŒ åªæ”¯æŒNVIDIA GPU
âŒ éƒ¨ç½²å¤æ‚
âŒ éœ€è¦ç¼–è¯‘æ¨¡åž‹
```

#### llama.cppï¼ˆCPUæŽ¨ç†ï¼‰

```
åœ¨CPUä¸Šè¿è¡Œå¤§æ¨¡åž‹

ä¼˜åŠ¿ï¼š
âœ… ä¸éœ€è¦GPU
âœ… æ”¯æŒMac M1/M2
âœ… é‡åŒ–åˆ°2/3/4bit
âœ… è½»é‡çº§

åŠ£åŠ¿ï¼š
âŒ é€Ÿåº¦æ¯”GPUæ…¢10-20å€
```

---

## ðŸ” RAGæŠ€æœ¯æ ˆ

### Native RAGï¼ˆæ‰‹æ“RAGï¼‰

**åŸºç¡€æµç¨‹**ï¼š

```
RAG = Retrieval (æ£€ç´¢) + Augmented (å¢žå¼º) + Generation (ç”Ÿæˆ)

å®Œæ•´æµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. æ•°æ®å‡†å¤‡ â”‚
â”‚ â”œâ”€ æ”¶é›†æ–‡æ¡£  â”‚
â”‚ â”œâ”€ åˆ‡åˆ†æ–‡æ¡£  â”‚  â† Chunking
â”‚ â””â”€ å‘é‡åŒ–    â”‚  â† Embedding
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å‘é‡å­˜å‚¨ â”‚
â”‚ â””â”€ å‘é‡æ•°æ®åº“â”‚  â† Milvus/Chroma
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ç”¨æˆ·æé—® â”‚
â”‚ â””â”€ Query   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. æ£€ç´¢ç›¸å…³ â”‚
â”‚ â””â”€ Top-K   â”‚  â† ç›¸ä¼¼åº¦æœç´¢
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. æž„å»ºPromptâ”‚
â”‚ â””â”€ Context â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. LLMç”Ÿæˆ â”‚
â”‚ â””â”€ Answer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### æ‰‹æ“RAGå®žçŽ°

```python
# æœ€ç®€å•çš„Native RAGå®žçŽ°

import numpy as np
from openai import OpenAI

client = OpenAI(api_key="your_key")

# 1. å‡†å¤‡æ–‡æ¡£
documents = [
    "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œç®€å•æ˜“å­¦ã€‚",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥žç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚",
    "Transformeræ˜¯ä¸€ç§ç¥žç»ç½‘ç»œæž¶æž„ã€‚"
]

# 2. æ–‡æ¡£å‘é‡åŒ–
def get_embedding(text):
    response = client.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    return np.array(response.data[0].embedding)

doc_embeddings = [get_embedding(doc) for doc in documents]

# 3. ç›¸ä¼¼åº¦æœç´¢
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def retrieve(query, top_k=2):
    query_embedding = get_embedding(query)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = [
        cosine_similarity(query_embedding, doc_emb)
        for doc_emb in doc_embeddings
    ]
    
    # æŽ’åºèŽ·å–Top-K
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    
    return [documents[i] for i in top_indices]

# 4. RAGç”Ÿæˆ
def rag_answer(question):
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    relevant_docs = retrieve(question, top_k=2)
    
    # æž„å»ºPrompt
    context = "\n".join(relevant_docs)
    prompt = f"""
    å‚è€ƒä»¥ä¸‹èµ„æ–™å›žç­”é—®é¢˜ï¼š
    
    {context}
    
    é—®é¢˜ï¼š{question}
    
    è¯·åŸºäºŽä¸Šè¿°èµ„æ–™å›žç­”ã€‚
    """
    
    # LLMç”Ÿæˆ
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response.choices[0].message.content

# ä½¿ç”¨
answer = rag_answer("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
print(answer)
```

---

### LangChainå®žçŽ°RAG

**LangChain**ï¼šæœ€æµè¡Œçš„RAGæ¡†æž¶

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("documents.txt")
documents = loader.load()

# 2. åˆ‡åˆ†æ–‡æ¡£
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = text_splitter.split_documents(documents)

# 3. å‘é‡åŒ–å¹¶å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings
)

# 4. åˆ›å»ºRAGé“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

# 5. æé—®
question = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
answer = qa_chain.run(question)
print(answer)
```

**LangChainé«˜çº§ç‰¹æ€§**ï¼š

```python
# 1. å¤šç§Retriever
from langchain.retrievers import (
    BM25Retriever,  # å…³é”®è¯æ£€ç´¢
    EnsembleRetriever,  # æ··åˆæ£€ç´¢
    ContextualCompressionRetriever  # åŽ‹ç¼©æ£€ç´¢
)

# 2. Re-rankingï¼ˆé‡æŽ’åºï¼‰
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)

# 3. å¯¹è¯è®°å¿†
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(),
    memory=memory
)

# å¤šè½®å¯¹è¯
qa_chain("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
qa_chain("å®ƒæœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ")  # "å®ƒ"ä¼šç†è§£ä¸º"æ·±åº¦å­¦ä¹ "
```

---

### LlamaIndexå®žçŽ°RAG

**LlamaIndex**ï¼šä¸“æ³¨äºŽæ•°æ®ç´¢å¼•å’Œæ£€ç´¢

```python
from llama_index import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    ServiceContext
)
from llama_index.llms import OpenAI

# 1. åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# 2. åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“Ž
query_engine = index.as_query_engine()

# 4. æŸ¥è¯¢
response = query_engine.query("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
print(response)
```

**LlamaIndexé«˜çº§ç´¢å¼•**ï¼š

```python
# 1. æ ‘å½¢ç´¢å¼•ï¼ˆTree Indexï¼‰
from llama_index import TreeIndex

tree_index = TreeIndex.from_documents(documents)

# 2. å…³é”®è¯ç´¢å¼•ï¼ˆKeyword Indexï¼‰
from llama_index import KeywordTableIndex

keyword_index = KeywordTableIndex.from_documents(documents)

# 3. åˆ—è¡¨ç´¢å¼•ï¼ˆList Indexï¼‰
from llama_index import ListIndex

list_index = ListIndex.from_documents(documents)

# 4. çŸ¥è¯†å›¾è°±ç´¢å¼•
from llama_index import KnowledgeGraphIndex

kg_index = KnowledgeGraphIndex.from_documents(documents)
```

---

### GraphRAGè¯¦è§£

**GraphRAG = å›¾ + RAG**

ä¼ ç»ŸRAGé—®é¢˜ï¼š
```
ä¼ ç»ŸRAGï¼š
ç”¨æˆ·: "æ€»ç»“è¿™äº›æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹"
ç³»ç»Ÿ: 
1. æ£€ç´¢ç›¸å…³chunks
2. æ¯ä¸ªchunkç‹¬ç«‹
3. éš¾ä»¥æŠŠæ¡å…¨å±€

é—®é¢˜ï¼š
âŒ ç¼ºä¹å…¨å±€è§†è§’
âŒ æ— æ³•è¿žæŽ¥ç›¸å…³ä¿¡æ¯
âŒ éš¾ä»¥æ€»ç»“æ•´ä½“
```

**GraphRAGè§£å†³æ–¹æ¡ˆ**ï¼š

```
GraphRAGæµç¨‹ï¼š
1. æž„å»ºçŸ¥è¯†å›¾è°±
   â”œâ”€ æå–å®žä½“
   â”œâ”€ æå–å…³ç³»
   â””â”€ æž„å»ºå›¾ç»“æž„

2. ç¤¾åŒºæ£€æµ‹
   â””â”€ è¯†åˆ«ä¸»é¢˜ç¤¾åŒº

3. ç”Ÿæˆæ‘˜è¦
   â”œâ”€ ç¤¾åŒºçº§æ‘˜è¦
   â””â”€ å…¨å±€æ‘˜è¦

4. æŸ¥è¯¢æ—¶
   â”œâ”€ å›¾æœç´¢
   â”œâ”€ ç¤¾åŒºæ£€ç´¢
   â””â”€ å…¨å±€æŽ¨ç†
```

**GraphRAGæž¶æž„**ï¼š

```
æ–‡æ¡£é›†åˆ
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. å®žä½“å…³ç³»æå–      â”‚
â”‚                     â”‚
â”‚ æ–‡æ¡£1: "å°æ˜Žåœ¨åŒ—äº¬  â”‚
â”‚        å·¥ä½œäºŽè…¾è®¯"  â”‚
â”‚    â†“                â”‚
â”‚ å®žä½“: [å°æ˜Ž, åŒ—äº¬,  â”‚
â”‚       è…¾è®¯]         â”‚
â”‚ å…³ç³»: å°æ˜Ž-å·¥ä½œäºŽ-   â”‚
â”‚      è…¾è®¯           â”‚
â”‚      å°æ˜Ž-ä½äºŽ-åŒ—äº¬  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. æž„å»ºçŸ¥è¯†å›¾è°±      â”‚
â”‚                     â”‚
â”‚   [å°æ˜Ž]            â”‚
â”‚    â†™  â†˜             â”‚
â”‚ å·¥ä½œäºŽ  ä½äºŽ         â”‚
â”‚  â†™      â†˜          â”‚
â”‚[è…¾è®¯]  [åŒ—äº¬]       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ç¤¾åŒºæ£€æµ‹          â”‚
â”‚                     â”‚
â”‚ ç¤¾åŒº1: äººç‰©ç›¸å…³      â”‚
â”‚ â””â”€ [å°æ˜Ž, æŽåŽ...]  â”‚
â”‚                     â”‚
â”‚ ç¤¾åŒº2: å…¬å¸ç›¸å…³      â”‚
â”‚ â””â”€ [è…¾è®¯, é˜¿é‡Œ...]  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ç”Ÿæˆæ‘˜è¦          â”‚
â”‚                     â”‚
â”‚ ç¤¾åŒº1æ‘˜è¦:          â”‚
â”‚ "è¿™äº›äººç‰©ä¸»è¦æ˜¯..."  â”‚
â”‚                     â”‚
â”‚ å…¨å±€æ‘˜è¦:           â”‚
â”‚ "æ•´ä½“ä¸Šï¼Œæ–‡æ¡£è®¨è®º..." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GraphRAGå®žçŽ°**ï¼š

```python
# Microsoft GraphRAG
from graphrag import GraphRAG

# 1. åˆå§‹åŒ–
graphrag = GraphRAG(
    llm_model="gpt-4",
    embedding_model="text-embedding-ada-002"
)

# 2. ç´¢å¼•æ–‡æ¡£
graphrag.index_documents([
    "å°æ˜Žåœ¨åŒ—äº¬å·¥ä½œäºŽè…¾è®¯ã€‚",
    "æŽåŽåœ¨ä¸Šæµ·å·¥ä½œäºŽé˜¿é‡Œã€‚",
    "è…¾è®¯æ˜¯ä¸€å®¶äº’è”ç½‘å…¬å¸ã€‚"
])

# 3. å…¨å±€æŸ¥è¯¢
response = graphrag.query(
    "æ€»ç»“è¿™äº›æ–‡æ¡£ä¸­çš„äººç‰©å’Œå…¬å¸",
    query_type="global"  # å…¨å±€æŸ¥è¯¢
)

# 4. å±€éƒ¨æŸ¥è¯¢
response = graphrag.query(
    "å°æ˜Žåœ¨å“ªå·¥ä½œï¼Ÿ",
    query_type="local"  # å±€éƒ¨æŸ¥è¯¢
)
```

---

### Agentic RAG

**Agentic RAG = Agent + RAG**

æ ¸å¿ƒæ€æƒ³ï¼šRAGä¸æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œè€Œæ˜¯å¤šæ­¥æŽ¨ç†

```
ä¼ ç»ŸRAG:
é—®é¢˜ â†’ æ£€ç´¢ â†’ ç”Ÿæˆ â†’ ç­”æ¡ˆ
      (ä¸€æ¬¡æ€§)

Agentic RAG:
é—®é¢˜ â†’ Agentå†³ç­–
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚ æ£€ç´¢1 â”‚
    â””â”€â”€â”€â”¬â”€â”€â”˜
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚ åˆ†æž  â”‚ â† éœ€è¦æ›´å¤šä¿¡æ¯å—ï¼Ÿ
    â””â”€â”€â”€â”¬â”€â”€â”˜
        â†“ æ˜¯
    â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚ æ£€ç´¢2 â”‚ â† æ¢ä¸ªè§’åº¦æ£€ç´¢
    â””â”€â”€â”€â”¬â”€â”€â”˜
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚ ç»¼åˆ  â”‚
    â””â”€â”€â”€â”¬â”€â”€â”˜
        â†“
      ç­”æ¡ˆ
```

**Agentic RAGå®žçŽ°**ï¼š

```python
from langchain.agents import Tool, AgentExecutor
from langchain.agents import create_react_agent
from langchain.llms import OpenAI

# 1. å®šä¹‰å·¥å…·
retriever_tool = Tool(
    name="Document Search",
    func=vectorstore.similarity_search,
    description="æœç´¢ç›¸å…³æ–‡æ¡£"
)

calculator_tool = Tool(
    name="Calculator",
    func=lambda x: eval(x),
    description="æ•°å­¦è®¡ç®—"
)

# 2. åˆ›å»ºAgent
agent = create_react_agent(
    llm=OpenAI(temperature=0),
    tools=[retriever_tool, calculator_tool]
)

executor = AgentExecutor(agent=agent, tools=[...])

# 3. æ‰§è¡Œ
result = executor.run(
    "æ ¹æ®æ–‡æ¡£ï¼Œè®¡ç®—åŽ»å¹´å’Œä»Šå¹´çš„è¥æ”¶å¢žé•¿çŽ‡"
)

# Agentä¼šï¼š
# Step 1: ç”¨retriever_toolæœç´¢"åŽ»å¹´è¥æ”¶"
# Step 2: ç”¨retriever_toolæœç´¢"ä»Šå¹´è¥æ”¶"
# Step 3: ç”¨calculator_toolè®¡ç®—å¢žé•¿çŽ‡
# Step 4: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
```

---

### RAGæŠ€æœ¯å¯¹æ¯”

| æŠ€æœ¯               | ä¼˜åŠ¿       | åŠ£åŠ¿       | é€‚ç”¨åœºæ™¯     |
| ------------------ | ---------- | ---------- | ------------ |
| **Native RAG**     | ç®€å•ç›´æŽ¥   | åŠŸèƒ½æœ‰é™   | å­¦ä¹ ã€åŽŸåž‹   |
| **LangChain RAG**  | åŠŸèƒ½ä¸°å¯Œ   | è¾ƒå¤æ‚     | é€šç”¨RAGâ­     |
| **LlamaIndex RAG** | ç´¢å¼•å¼ºå¤§   | å­¦ä¹ æ›²çº¿é™¡ | å¤æ‚æ•°æ®ç»“æž„ |
| **GraphRAG**       | å…¨å±€ç†è§£   | æˆæœ¬é«˜     | éœ€è¦å…¨å±€è§†è§’ |
| **Agentic RAG**    | æŽ¨ç†èƒ½åŠ›å¼º | æœ€å¤æ‚     | å¤æ‚é—®ç­”â­    |

---

## ðŸŽ¯ å­¦ä¹ è·¯çº¿å»ºè®®

### æ–°æ‰‹è·¯çº¿ï¼ˆ0-3ä¸ªæœˆï¼‰

```
Week 1-2: TransformeråŸºç¡€
â”œâ”€ è®ºæ–‡ï¼šã€ŠAttention is All You Needã€‹
â”œâ”€ è§†é¢‘ï¼š3Blue1Brownçš„Transformerè®²è§£
â””â”€ ä»£ç ï¼šä»Žé›¶å®žçŽ°Mini-Transformer

Week 3-4: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹
â”œâ”€ Hugging Face Transformers
â”œâ”€ åŠ è½½Qwen/LLaMA
â””â”€ ç®€å•æŽ¨ç†

Week 5-6: LoRAå¾®è°ƒ
â”œâ”€ ç†è§£LoRAåŽŸç†
â”œâ”€ ä½¿ç”¨LlamaFactory
â””â”€ å¾®è°ƒè‡ªå·±çš„æ¨¡åž‹

Week 7-8: æŽ¨ç†åŠ é€Ÿ
â”œâ”€ vLLMå®‰è£…ä½¿ç”¨
â”œâ”€ éƒ¨ç½²APIæœåŠ¡
â””â”€ æ€§èƒ½æµ‹è¯•

Week 9-10: RAGå…¥é—¨
â”œâ”€ æ‰‹æ“Native RAG
â”œâ”€ LangChain RAG
â””â”€ å®žæˆ˜é¡¹ç›®

Week 11-12: è¿›é˜¶é¡¹ç›®
â””â”€ å®Œæ•´çš„RAGåº”ç”¨
```

### è¿›é˜¶è·¯çº¿ï¼ˆ3-6ä¸ªæœˆï¼‰

```
æ·±å…¥æ¨¡åž‹æž¶æž„
â”œâ”€ DeepSeek MoE
â”œâ”€ Qwen 2.5æ”¹è¿›
â””â”€ ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰

é«˜çº§å¾®è°ƒæŠ€æœ¯
â”œâ”€ QLoRA
â”œâ”€ Full Fine-tuning
â””â”€ RLHF

ç”Ÿäº§çº§éƒ¨ç½²
â”œâ”€ SGLang
â”œâ”€ TensorRT-LLM
â””â”€ åˆ†å¸ƒå¼æŽ¨ç†

é«˜çº§RAG
â”œâ”€ GraphRAG
â”œâ”€ Agentic RAG
â””â”€ å¤šæ¨¡æ€RAG
```

---

## ðŸ“š èµ„æºæŽ¨è

### è®ºæ–‡

- Attention is All You Need (Transformer)
- LoRA: Low-Rank Adaptation
- Efficient Streaming LLM (PagedAttention)
- GraphRAG: Graph-based RAG

### å¼€æºé¡¹ç›®

- **LlamaFactory**: https://github.com/hiyouga/LLaMA-Factory
- **vLLM**: https://github.com/vllm-project/vllm
- **SGLang**: https://github.com/sgl-project/sglang
- **LangChain**: https://github.com/langchain-ai/langchain
- **LlamaIndex**: https://github.com/run-llama/llama_index

### å­¦ä¹ èµ„æº

- Hugging Face Course
- DeepLearning.AI Short Courses
- Stanford CS224N (NLP)
- Fast.ai Course

---

**ç¥ä½ å­¦ä¹ é¡ºåˆ©ï¼** ðŸš€

