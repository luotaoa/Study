# åˆåŒå®¡æŸ¥AIé¡¹ç›®æŠ€æœ¯æ ˆè¯¦è§£

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®åŠŸèƒ½æ¦‚è¿°](#é¡¹ç›®åŠŸèƒ½æ¦‚è¿°)
2. [æ ¸å¿ƒæŠ€æœ¯æ ˆ](#æ ¸å¿ƒæŠ€æœ¯æ ˆ)
3. [æ–‡æ¡£è§£ææŠ€æœ¯](#æ–‡æ¡£è§£ææŠ€æœ¯)
4. [NLPä¸ä¿¡æ¯æŠ½å–](#nlpä¸ä¿¡æ¯æŠ½å–)
5. [å¤§æ¨¡å‹åº”ç”¨](#å¤§æ¨¡å‹åº”ç”¨)
6. [åˆåŒå¯¹æ¯”æŠ€æœ¯](#åˆåŒå¯¹æ¯”æŠ€æœ¯)
7. [å®Œæ•´æ¶æ„è®¾è®¡](#å®Œæ•´æ¶æ„è®¾è®¡)
8. [å¼€æºé¡¹ç›®æ¨è](#å¼€æºé¡¹ç›®æ¨è)

---

## ğŸ¯ é¡¹ç›®åŠŸèƒ½æ¦‚è¿°

### åˆåŒå®¡æŸ¥ç³»ç»Ÿé€šå¸¸åŒ…å«

```
æ ¸å¿ƒåŠŸèƒ½ï¼š
â”œâ”€ ğŸ“„ æ–‡æ¡£ä¸Šä¼ ï¼ˆPDFã€Wordã€å›¾ç‰‡ï¼‰
â”œâ”€ ğŸ” å…³é”®ä¿¡æ¯æå–
â”‚   â”œâ”€ åˆåŒä¸»ä½“ï¼ˆç”²æ–¹ã€ä¹™æ–¹ï¼‰
â”‚   â”œâ”€ é‡‘é¢æ¡æ¬¾
â”‚   â”œâ”€ æ—¥æœŸæ¡æ¬¾ï¼ˆç­¾è®¢æ—¥æœŸã€ç”Ÿæ•ˆæ—¥æœŸã€åˆ°æœŸæ—¥æœŸï¼‰
â”‚   â”œâ”€ æƒåˆ©ä¹‰åŠ¡æ¡æ¬¾
â”‚   â””â”€ è¿çº¦è´£ä»»æ¡æ¬¾
â”œâ”€ âš ï¸ é£é™©ç‚¹è¯†åˆ«
â”‚   â”œâ”€ æ³•å¾‹é£é™©ï¼ˆä¸ç¬¦åˆæ³•å¾‹æ³•è§„ï¼‰
â”‚   â”œâ”€ å•†ä¸šé£é™©ï¼ˆæ¡æ¬¾å¯¹å·±æ–¹ä¸åˆ©ï¼‰
â”‚   â”œâ”€ é—æ¼é£é™©ï¼ˆç¼ºå°‘å¿…è¦æ¡æ¬¾ï¼‰
â”‚   â””â”€ æ­§ä¹‰é£é™©ï¼ˆè¡¨è¿°ä¸æ¸…ï¼‰
â”œâ”€ ğŸ“Š åˆåŒå¯¹æ¯”
â”‚   â”œâ”€ ä¸¤ç‰ˆæœ¬å¯¹æ¯”ï¼ˆçº¢çº¿æ ‡æ³¨ï¼‰
â”‚   â”œâ”€ ä¸æ ‡å‡†æ¨¡æ¿å¯¹æ¯”
â”‚   â””â”€ å·®å¼‚ç‚¹åˆ†æ
â”œâ”€ ğŸ’¡ ä¿®æ”¹å»ºè®®
â”‚   â””â”€ ç»™å‡ºå…·ä½“çš„ä¿®æ”¹æ„è§
â””â”€ ğŸ“ˆ å®¡æŸ¥æŠ¥å‘Šç”Ÿæˆ
```

---

## ğŸ› ï¸ æ ¸å¿ƒæŠ€æœ¯æ ˆ

### æŠ€æœ¯æ¶æ„å…¨æ™¯å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          å‰ç«¯å±‚                         â”‚
â”‚  React/Vue + Ant Design                 â”‚
â”‚  + PDF.js/Viewerï¼ˆæ–‡æ¡£é¢„è§ˆï¼‰            â”‚
â”‚  + Diffåº“ï¼ˆå¯¹æ¯”å±•ç¤ºï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          APIå±‚                          â”‚
â”‚  FastAPI/Django/Node.js                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       æ–‡æ¡£å¤„ç†å±‚                        â”‚
â”‚  â”œâ”€ PDFè§£æï¼šPyPDF2/pdfplumber         â”‚
â”‚  â”œâ”€ Wordè§£æï¼špython-docx               â”‚
â”‚  â”œâ”€ OCRï¼šPaddleOCR/Tesseract           â”‚
â”‚  â””â”€ è¡¨æ ¼æå–ï¼šCamelot/Tabula           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       NLPå¤„ç†å±‚                         â”‚
â”‚  â”œâ”€ å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰                 â”‚
â”‚  â”œâ”€ å…³ç³»æŠ½å–ï¼ˆREï¼‰                      â”‚
â”‚  â”œâ”€ æ–‡æœ¬åˆ†ç±»                            â”‚
â”‚  â””â”€ ç›¸ä¼¼åº¦è®¡ç®—                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å¤§æ¨¡å‹å±‚                          â”‚
â”‚  â”œâ”€ GPT-4/Claudeï¼ˆç†è§£+ç”Ÿæˆï¼‰          â”‚
â”‚  â”œâ”€ Qwen-72Bï¼ˆå¼€æºæ–¹æ¡ˆï¼‰               â”‚
â”‚  â””â”€ æ³•å¾‹ä¸“ç”¨æ¨¡å‹ï¼ˆLaWGPT/æ™ºæµ·ï¼‰        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       çŸ¥è¯†åº“å±‚ï¼ˆRAGï¼‰                   â”‚
â”‚  â”œâ”€ å‘é‡æ•°æ®åº“ï¼šMilvus/Chroma          â”‚
â”‚  â”œâ”€ æ³•å¾‹æ³•è§„åº“                          â”‚
â”‚  â”œâ”€ åˆ¤ä¾‹åº“                              â”‚
â”‚  â””â”€ ä¼ä¸šåˆåŒæ¨¡æ¿åº“                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å­˜å‚¨å±‚                            â”‚
â”‚  â”œâ”€ æ–‡æ¡£å­˜å‚¨ï¼šMinIO/OSS                â”‚
â”‚  â”œâ”€ æ•°æ®åº“ï¼šPostgreSQL/MongoDB         â”‚
â”‚  â””â”€ ç¼“å­˜ï¼šRedis                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“„ ç¬¬ä¸€å±‚ï¼šæ–‡æ¡£è§£ææŠ€æœ¯

### 1. PDFè§£æ

**é—®é¢˜**ï¼šåˆåŒé€šå¸¸æ˜¯æ‰«æä»¶æˆ–PDFæ ¼å¼

```python
# æ–¹æ¡ˆ1ï¼šçº¯æ•°å­—PDFï¼ˆå¯å¤åˆ¶æ–‡å­—çš„PDFï¼‰
import pdfplumber

def extract_pdf_text(pdf_path):
    """æå–PDFæ–‡å­—å†…å®¹"""
    text = ""
    tables = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            # æå–æ–‡å­—
            text += page.extract_text()
            
            # æå–è¡¨æ ¼
            page_tables = page.extract_tables()
            if page_tables:
                tables.extend(page_tables)
    
    return text, tables

# ä½¿ç”¨
text, tables = extract_pdf_text("åˆåŒ.pdf")
print(f"æå–æ–‡å­—ï¼š{len(text)}å­—")
print(f"æå–è¡¨æ ¼ï¼š{len(tables)}ä¸ª")
```

```python
# æ–¹æ¡ˆ2ï¼šæ‰«æä»¶PDFï¼ˆéœ€è¦OCRï¼‰
from pdf2image import convert_from_path
from paddleocr import PaddleOCR
import cv2

class ScanPDFParser:
    def __init__(self):
        # åˆå§‹åŒ–OCRå¼•æ“
        self.ocr = PaddleOCR(
            use_angle_cls=True,  # æ–‡å­—æ–¹å‘æ£€æµ‹
            lang='ch',           # ä¸­æ–‡
            use_gpu=True         # ä½¿ç”¨GPUåŠ é€Ÿ
        )
    
    def parse_scan_pdf(self, pdf_path):
        """è§£ææ‰«æç‰ˆPDF"""
        # 1. PDFè½¬å›¾ç‰‡
        images = convert_from_path(pdf_path, dpi=300)
        
        all_text = ""
        for i, image in enumerate(images):
            print(f"å¤„ç†ç¬¬{i+1}é¡µ...")
            
            # 2. å›¾ç‰‡é¢„å¤„ç†ï¼ˆæé«˜OCRå‡†ç¡®ç‡ï¼‰
            processed = self.preprocess_image(image)
            
            # 3. OCRè¯†åˆ«
            result = self.ocr.ocr(processed, cls=True)
            
            # 4. æå–æ–‡å­—
            page_text = self.extract_text_from_ocr(result)
            all_text += f"\nç¬¬{i+1}é¡µ\n{page_text}\n"
        
        return all_text
    
    def preprocess_image(self, image):
        """å›¾ç‰‡é¢„å¤„ç†"""
        import numpy as np
        
        # PILè½¬OpenCV
        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # ç°åº¦åŒ–
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # äºŒå€¼åŒ–ï¼ˆæé«˜å¯¹æ¯”åº¦ï¼‰
        _, binary = cv2.threshold(
            gray, 0, 255, 
            cv2.THRESH_BINARY + cv2.THRESH_OTSU
        )
        
        # å»å™ª
        denoised = cv2.fastNlMeansDenoising(binary)
        
        return denoised
    
    def extract_text_from_ocr(self, ocr_result):
        """ä»OCRç»“æœæå–æ–‡å­—"""
        text = ""
        for line in ocr_result:
            for word_info in line:
                text += word_info[1][0] + " "  # word_info[1][0]æ˜¯è¯†åˆ«çš„æ–‡å­—
            text += "\n"
        return text

# ä½¿ç”¨
parser = ScanPDFParser()
text = parser.parse_scan_pdf("æ‰«æç‰ˆåˆåŒ.pdf")
```

### 2. Wordæ–‡æ¡£è§£æ

```python
from docx import Document

def extract_word_content(docx_path):
    """æå–Wordæ–‡æ¡£å†…å®¹"""
    doc = Document(docx_path)
    
    # æå–æ®µè½
    paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
    
    # æå–è¡¨æ ¼
    tables = []
    for table in doc.tables:
        table_data = []
        for row in table.rows:
            row_data = [cell.text for cell in row.cells]
            table_data.append(row_data)
        tables.append(table_data)
    
    return {
        'text': '\n'.join(paragraphs),
        'tables': tables
    }

# ä½¿ç”¨
content = extract_word_content("åˆåŒ.docx")
```

### 3. é€šç”¨æ–‡æ¡£è§£ææ¡†æ¶

**æ¨èï¼šUnstructured** - ä¸€ç«™å¼æ–‡æ¡£è§£æ

```python
from unstructured.partition.auto import partition

def parse_any_document(file_path):
    """è§£æä»»ä½•æ ¼å¼çš„æ–‡æ¡£"""
    # è‡ªåŠ¨è¯†åˆ«æ ¼å¼å¹¶è§£æ
    elements = partition(filename=file_path)
    
    # æå–ä¸åŒç±»å‹çš„å†…å®¹
    result = {
        'title': [],
        'text': [],
        'tables': [],
        'lists': []
    }
    
    for element in elements:
        element_type = type(element).__name__
        
        if element_type == 'Title':
            result['title'].append(str(element))
        elif element_type == 'NarrativeText':
            result['text'].append(str(element))
        elif element_type == 'Table':
            result['tables'].append(str(element))
        elif element_type == 'ListItem':
            result['lists'].append(str(element))
    
    return result

# æ”¯æŒçš„æ ¼å¼
# PDFã€Wordã€Excelã€PPTã€HTMLã€å›¾ç‰‡ç­‰
content = parse_any_document("åˆåŒ.pdf")
```

---

## ğŸ§  ç¬¬äºŒå±‚ï¼šNLPä¸ä¿¡æ¯æŠ½å–

### 1. å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰- æå–å…³é”®ä¿¡æ¯

**ç›®æ ‡**ï¼šä»åˆåŒä¸­æå–ç”²æ–¹ã€ä¹™æ–¹ã€é‡‘é¢ã€æ—¥æœŸç­‰

```python
from transformers import pipeline

class ContractNER:
    def __init__(self):
        # ä½¿ç”¨æ³•å¾‹é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹
        self.ner = pipeline(
            "ner", 
            model="law-ner-model",  # æ³•å¾‹é¢†åŸŸNERæ¨¡å‹
            aggregation_strategy="simple"
        )
    
    def extract_entities(self, text):
        """æå–åˆåŒå®ä½“"""
        # NERè¯†åˆ«
        entities = self.ner(text)
        
        # åˆ†ç±»æ•´ç†
        result = {
            'ç”²æ–¹': [],
            'ä¹™æ–¹': [],
            'é‡‘é¢': [],
            'æ—¥æœŸ': [],
            'åœ°ç‚¹': []
        }
        
        for entity in entities:
            entity_type = entity['entity_group']
            entity_text = entity['word']
            
            if entity_type == 'PARTY_A':
                result['ç”²æ–¹'].append(entity_text)
            elif entity_type == 'PARTY_B':
                result['ä¹™æ–¹'].append(entity_text)
            elif entity_type == 'MONEY':
                result['é‡‘é¢'].append(entity_text)
            elif entity_type == 'DATE':
                result['æ—¥æœŸ'].append(entity_text)
            elif entity_type == 'LOCATION':
                result['åœ°ç‚¹'].append(entity_text)
        
        return result

# ä½¿ç”¨
ner = ContractNER()
entities = ner.extract_entities(contract_text)
print(entities)
# è¾“å‡ºï¼š
# {
#   'ç”²æ–¹': ['åŒ—äº¬XXç§‘æŠ€æœ‰é™å…¬å¸'],
#   'ä¹™æ–¹': ['ä¸Šæµ·XXè´¸æ˜“æœ‰é™å…¬å¸'],
#   'é‡‘é¢': ['100ä¸‡å…ƒ', '10ä¸‡å…ƒ'],
#   'æ—¥æœŸ': ['2024å¹´1æœˆ1æ—¥', '2025å¹´12æœˆ31æ—¥'],
#   'åœ°ç‚¹': ['åŒ—äº¬å¸‚æœé˜³åŒº']
# }
```

### 2. æ¡æ¬¾åˆ†ç±»

**ç›®æ ‡**ï¼šå°†åˆåŒåˆ†æˆä¸åŒçš„æ¡æ¬¾ç±»å‹

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class ClauseClassifier:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("legal-bert")
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "legal-clause-classifier"
        )
        
        self.labels = [
            'åˆåŒä¸»ä½“',
            'åˆåŒæ ‡çš„',
            'ä»·æ¬¾æ”¯ä»˜',
            'å±¥è¡ŒæœŸé™',
            'è¿çº¦è´£ä»»',
            'äº‰è®®è§£å†³',
            'å…¶ä»–'
        ]
    
    def classify_clause(self, clause_text):
        """åˆ†ç±»å•ä¸ªæ¡æ¬¾"""
        inputs = self.tokenizer(
            clause_text, 
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        probs = torch.softmax(outputs.logits, dim=-1)
        label_id = torch.argmax(probs, dim=-1).item()
        confidence = probs[0][label_id].item()
        
        return {
            'label': self.labels[label_id],
            'confidence': confidence
        }
    
    def split_and_classify(self, contract_text):
        """æ‹†åˆ†å¹¶åˆ†ç±»æ‰€æœ‰æ¡æ¬¾"""
        # 1. æŒ‰æ¡æ¬¾ç¼–å·æ‹†åˆ†
        clauses = self.split_clauses(contract_text)
        
        # 2. åˆ†ç±»æ¯ä¸ªæ¡æ¬¾
        results = []
        for clause in clauses:
            classification = self.classify_clause(clause['text'])
            results.append({
                'clause_number': clause['number'],
                'text': clause['text'],
                'type': classification['label'],
                'confidence': classification['confidence']
            })
        
        return results
    
    def split_clauses(self, text):
        """æŒ‰æ¡æ¬¾ç¼–å·æ‹†åˆ†"""
        import re
        
        # åŒ¹é…"ç¬¬Xæ¡"ã€"ç¬¬Xç« "ç­‰
        pattern = r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡'
        matches = list(re.finditer(pattern, text))
        
        clauses = []
        for i, match in enumerate(matches):
            start = match.start()
            end = matches[i+1].start() if i+1 < len(matches) else len(text)
            
            clauses.append({
                'number': match.group(),
                'text': text[start:end].strip()
            })
        
        return clauses

# ä½¿ç”¨
classifier = ClauseClassifier()
classified_clauses = classifier.split_and_classify(contract_text)

for clause in classified_clauses:
    print(f"{clause['clause_number']}: {clause['type']} (ç½®ä¿¡åº¦:{clause['confidence']:.2f})")
```

---

## ğŸ¤– ç¬¬ä¸‰å±‚ï¼šå¤§æ¨¡å‹åº”ç”¨

### 1. ä½¿ç”¨GPT-4è¿›è¡ŒåˆåŒå®¡æŸ¥

```python
from openai import OpenAI

class ContractReviewer:
    def __init__(self):
        self.client = OpenAI(api_key="your_key")
    
    def review_contract(self, contract_text):
        """å®Œæ•´å®¡æŸ¥åˆåŒ"""
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ³•å¾‹é¡¾é—®ï¼Œè¯·å®¡æŸ¥ä»¥ä¸‹åˆåŒï¼Œå¹¶ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦ç»™å‡ºæ„è§ï¼š

åˆåŒå†…å®¹ï¼š
{contract_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

1. ã€åˆåŒæ¦‚è¦ã€‘
   - åˆåŒç±»å‹ï¼š
   - åˆåŒåŒæ–¹ï¼š
   - ä¸»è¦å†…å®¹ï¼š
   - åˆåŒé‡‘é¢ï¼š
   - å±¥è¡ŒæœŸé™ï¼š

2. ã€é£é™©ç‚¹è¯†åˆ«ã€‘
   åˆ—å‡ºæ‰€æœ‰æ½œåœ¨é£é™©ï¼Œæ¯ä¸ªé£é™©åŒ…æ‹¬ï¼š
   - é£é™©ç±»å‹ï¼ˆæ³•å¾‹é£é™©/å•†ä¸šé£é™©/é—æ¼é£é™©ï¼‰
   - é£é™©ç­‰çº§ï¼ˆé«˜/ä¸­/ä½ï¼‰
   - å…·ä½“æè¿°
   - æ¶‰åŠæ¡æ¬¾

3. ã€æ¡æ¬¾å»ºè®®ã€‘
   å¯¹æ¯ä¸ªæœ‰é—®é¢˜çš„æ¡æ¬¾ç»™å‡ºä¿®æ”¹å»ºè®®

4. ã€ç¼ºå¤±æ¡æ¬¾ã€‘
   åˆ—å‡ºåº”è¯¥æœ‰ä½†æ²¡æœ‰çš„æ¡æ¬¾

5. ã€æ€»ä½“è¯„ä»·ã€‘
   å¯¹åˆåŒçš„æ€»ä½“è¯„ä»·å’Œå»ºè®®
"""
        
        response = self.client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ³•å¾‹é¡¾é—®"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # é™ä½éšæœºæ€§ï¼Œæ›´ä¸¥è°¨
        )
        
        return response.choices[0].message.content
    
    def analyze_risk_clause(self, clause_text):
        """é’ˆå¯¹å•ä¸ªæ¡æ¬¾è¿›è¡Œé£é™©åˆ†æ"""
        prompt = f"""
åˆ†æä»¥ä¸‹åˆåŒæ¡æ¬¾çš„é£é™©ï¼š

æ¡æ¬¾å†…å®¹ï¼š
{clause_text}

è¯·åˆ†æï¼š
1. æ˜¯å¦å­˜åœ¨æ­§ä¹‰æˆ–æ¨¡ç³Šè¡¨è¿°ï¼Ÿ
2. æ˜¯å¦å¯¹æŸä¸€æ–¹æ˜æ˜¾ä¸åˆ©ï¼Ÿ
3. æ˜¯å¦ç¬¦åˆæ³•å¾‹æ³•è§„ï¼Ÿ
4. æ˜¯å¦å­˜åœ¨æ‰§è¡Œå›°éš¾ï¼Ÿ
5. å»ºè®®å¦‚ä½•ä¿®æ”¹ï¼Ÿ
"""
        
        response = self.client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        
        return response.choices[0].message.content

# ä½¿ç”¨
reviewer = ContractReviewer()
report = reviewer.review_contract(contract_text)
print(report)
```

### 2. ä½¿ç”¨RAGå¢å¼ºå®¡æŸ¥å‡†ç¡®æ€§

```python
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

class RAGContractReviewer:
    def __init__(self):
        # 1. åŠ è½½æ³•å¾‹çŸ¥è¯†åº“
        self.knowledge_base = self.build_knowledge_base()
        
    def build_knowledge_base(self):
        """æ„å»ºæ³•å¾‹çŸ¥è¯†åº“"""
        # åŠ è½½æ³•å¾‹æ–‡æ¡£
        loader = DirectoryLoader(
            './legal_docs/',  # æ³•å¾‹æ³•è§„ã€åˆ¤ä¾‹ã€æ¨¡æ¿
            glob="**/*.txt"
        )
        documents = loader.load()
        
        # åˆ‡åˆ†
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
        chunks = splitter.split_documents(documents)
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        vectorstore = Chroma.from_documents(
            chunks,
            OpenAIEmbeddings()
        )
        
        return vectorstore
    
    def review_with_rag(self, contract_text, clause_type):
        """åŸºäºRAGå®¡æŸ¥åˆåŒ"""
        # 1. æ£€ç´¢ç›¸å…³æ³•å¾‹æ¡æ–‡
        query = f"å…³äº{clause_type}çš„æ³•å¾‹è§„å®šå’Œæ³¨æ„äº‹é¡¹"
        relevant_docs = self.knowledge_base.similarity_search(query, k=5)
        
        # 2. æ„å»ºPrompt
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        prompt = f"""
å‚è€ƒä»¥ä¸‹æ³•å¾‹è§„å®šï¼š
{context}

è¯·å®¡æŸ¥ä»¥ä¸‹åˆåŒæ¡æ¬¾ï¼š
{contract_text}

è¯·æŒ‡å‡ºï¼š
1. æ˜¯å¦ç¬¦åˆæ³•å¾‹è§„å®š
2. æ˜¯å¦å­˜åœ¨é£é™©
3. å¦‚ä½•æ”¹è¿›
"""
        
        # 3. å¤§æ¨¡å‹åˆ†æ
        client = OpenAI(api_key="your_key")
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return {
            'analysis': response.choices[0].message.content,
            'references': [doc.metadata for doc in relevant_docs]
        }

# ä½¿ç”¨
rag_reviewer = RAGContractReviewer()
result = rag_reviewer.review_with_rag(
    contract_text="ç¬¬äº”æ¡ è¿çº¦è´£ä»»...",
    clause_type="è¿çº¦è´£ä»»"
)

print(result['analysis'])
print("\nå‚è€ƒä¾æ®ï¼š", result['references'])
```

---

## ğŸ“Š ç¬¬å››å±‚ï¼šåˆåŒå¯¹æ¯”æŠ€æœ¯

### 1. æ–‡æœ¬å±‚é¢å¯¹æ¯”ï¼ˆDiffç®—æ³•ï¼‰

```python
import difflib
from difflib import SequenceMatcher

class ContractComparator:
    def compare_contracts(self, old_text, new_text):
        """å¯¹æ¯”ä¸¤ä¸ªåˆåŒç‰ˆæœ¬"""
        # æŒ‰è¡Œå¯¹æ¯”
        old_lines = old_text.splitlines()
        new_lines = new_text.splitlines()
        
        # ç”ŸæˆDiff
        diff = difflib.unified_diff(
            old_lines, 
            new_lines,
            lineterm='',
            n=3  # ä¸Šä¸‹æ–‡è¡Œæ•°
        )
        
        # è§£æDiffç»“æœ
        changes = self.parse_diff(diff)
        
        return changes
    
    def parse_diff(self, diff):
        """è§£æDiffç»“æœ"""
        changes = {
            'added': [],      # æ–°å¢å†…å®¹
            'deleted': [],    # åˆ é™¤å†…å®¹
            'modified': []    # ä¿®æ”¹å†…å®¹
        }
        
        for line in diff:
            if line.startswith('+') and not line.startswith('+++'):
                changes['added'].append(line[1:])
            elif line.startswith('-') and not line.startswith('---'):
                changes['deleted'].append(line[1:])
        
        return changes
    
    def generate_html_diff(self, old_text, new_text):
        """ç”ŸæˆHTMLæ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š"""
        differ = difflib.HtmlDiff()
        html = differ.make_file(
            old_text.splitlines(),
            new_text.splitlines(),
            fromdesc='åŸç‰ˆæœ¬',
            todesc='æ–°ç‰ˆæœ¬',
            context=True,
            numlines=5
        )
        
        return html

# ä½¿ç”¨
comparator = ContractComparator()

old_contract = "ç¬¬ä¸€æ¡ ç”²æ–¹åº”åœ¨2024å¹´1æœˆ1æ—¥å‰æ”¯ä»˜100ä¸‡å…ƒ..."
new_contract = "ç¬¬ä¸€æ¡ ç”²æ–¹åº”åœ¨2024å¹´3æœˆ1æ—¥å‰æ”¯ä»˜120ä¸‡å…ƒ..."

changes = comparator.compare_contracts(old_contract, new_contract)
print("æ–°å¢ï¼š", changes['added'])
print("åˆ é™¤ï¼š", changes['deleted'])

# ç”ŸæˆHTMLå¯¹æ¯”æŠ¥å‘Š
html = comparator.generate_html_diff(old_contract, new_contract)
with open('diff_report.html', 'w', encoding='utf-8') as f:
    f.write(html)
```

### 2. è¯­ä¹‰å±‚é¢å¯¹æ¯”ï¼ˆåŸºäºEmbeddingï¼‰

```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

class SemanticComparator:
    def __init__(self):
        # ä½¿ç”¨æ³•å¾‹é¢†åŸŸçš„å¥å­ç¼–ç æ¨¡å‹
        self.model = SentenceTransformer('law-bert-base')
    
    def compare_clauses_semantic(self, old_clauses, new_clauses):
        """è¯­ä¹‰å±‚é¢å¯¹æ¯”æ¡æ¬¾"""
        # 1. è®¡ç®—æ‰€æœ‰æ¡æ¬¾çš„å‘é‡
        old_embeddings = self.model.encode(old_clauses)
        new_embeddings = self.model.encode(new_clauses)
        
        # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = util.cos_sim(old_embeddings, new_embeddings)
        
        # 3. æ‰¾å‡ºå˜åŒ–çš„æ¡æ¬¾
        changes = []
        
        for i, old_clause in enumerate(old_clauses):
            max_sim = similarity_matrix[i].max().item()
            most_similar_idx = similarity_matrix[i].argmax().item()
            
            if max_sim < 0.9:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                changes.append({
                    'old_clause': old_clause,
                    'new_clause': new_clauses[most_similar_idx],
                    'similarity': max_sim,
                    'change_type': self.classify_change(max_sim)
                })
        
        return changes
    
    def classify_change(self, similarity):
        """åˆ†ç±»å˜åŒ–ç¨‹åº¦"""
        if similarity < 0.5:
            return 'é‡å¤§ä¿®æ”¹'
        elif similarity < 0.8:
            return 'ä¸­ç­‰ä¿®æ”¹'
        else:
            return 'è½»å¾®ä¿®æ”¹'
    
    def highlight_differences(self, old_text, new_text):
        """é«˜äº®å·®å¼‚éƒ¨åˆ†"""
        # ç”¨å¤§æ¨¡å‹åˆ†æå…·ä½“å·®å¼‚
        from openai import OpenAI
        client = OpenAI()
        
        prompt = f"""
å¯¹æ¯”ä»¥ä¸‹ä¸¤æ®µæ–‡å­—ï¼ŒæŒ‡å‡ºå…·ä½“çš„å·®å¼‚ç‚¹ï¼š

åŸæ–‡ï¼š
{old_text}

æ–°æ–‡ï¼š
{new_text}

è¯·åˆ—å‡ºæ‰€æœ‰ä¿®æ”¹çš„åœ°æ–¹ï¼Œå¹¶è¯´æ˜ä¿®æ”¹çš„å½±å“ã€‚
"""
        
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.choices[0].message.content

# ä½¿ç”¨
comparator = SemanticComparator()

old_clauses = [
    "ç¬¬ä¸€æ¡ ç”²æ–¹åº”åœ¨åˆåŒç”Ÿæ•ˆå30æ—¥å†…æ”¯ä»˜100ä¸‡å…ƒ",
    "ç¬¬äºŒæ¡ ä¹™æ–¹åº”åœ¨æ”¶åˆ°æ¬¾é¡¹åå¼€å§‹ä¾›è´§"
]

new_clauses = [
    "ç¬¬ä¸€æ¡ ç”²æ–¹åº”åœ¨åˆåŒç­¾è®¢åä¸€ä¸ªæœˆå†…æ”¯ä»˜äººæ°‘å¸å£¹ä½°ä¸‡å…ƒæ•´",
    "ç¬¬äºŒæ¡ ä¹™æ–¹åº”åœ¨ç¡®è®¤åˆ°è´¦å3ä¸ªå·¥ä½œæ—¥å†…å‘è´§"
]

changes = comparator.compare_clauses_semantic(old_clauses, new_clauses)
for change in changes:
    print(f"ç›¸ä¼¼åº¦: {change['similarity']:.2f}")
    print(f"å˜åŒ–ç±»å‹: {change['change_type']}")
    print(f"åŸæ¡æ¬¾: {change['old_clause']}")
    print(f"æ–°æ¡æ¬¾: {change['new_clause']}")
    print("---")
```

### 3. ç»“æ„åŒ–å¯¹æ¯”ï¼ˆå…³é”®ä¿¡æ¯å¯¹æ¯”ï¼‰

```python
class StructuredComparator:
    def __init__(self):
        self.ner = ContractNER()  # å‰é¢å®šä¹‰çš„NER
    
    def compare_key_info(self, old_contract, new_contract):
        """å¯¹æ¯”å…³é”®ä¿¡æ¯"""
        # 1. æå–ä¸¤ä¸ªç‰ˆæœ¬çš„å…³é”®ä¿¡æ¯
        old_info = self.ner.extract_entities(old_contract)
        new_info = self.ner.extract_entities(new_contract)
        
        # 2. å¯¹æ¯”
        comparison = {}
        
        for key in old_info.keys():
            old_values = set(old_info[key])
            new_values = set(new_info[key])
            
            comparison[key] = {
                'åˆ é™¤': list(old_values - new_values),
                'æ–°å¢': list(new_values - old_values),
                'ä¿æŒ': list(old_values & new_values)
            }
        
        return comparison
    
    def generate_comparison_report(self, comparison):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = "# åˆåŒå¯¹æ¯”æŠ¥å‘Š\n\n"
        
        for entity_type, changes in comparison.items():
            if changes['åˆ é™¤'] or changes['æ–°å¢']:
                report += f"## {entity_type}\n\n"
                
                if changes['åˆ é™¤']:
                    report += f"âŒ åˆ é™¤: {', '.join(changes['åˆ é™¤'])}\n\n"
                
                if changes['æ–°å¢']:
                    report += f"âœ… æ–°å¢: {', '.join(changes['æ–°å¢'])}\n\n"
                
                if changes['ä¿æŒ']:
                    report += f"â¡ï¸ ä¿æŒ: {', '.join(changes['ä¿æŒ'])}\n\n"
        
        return report

# ä½¿ç”¨
structured = StructuredComparator()
comparison = structured.compare_key_info(old_contract, new_contract)
report = structured.generate_comparison_report(comparison)
print(report)

# è¾“å‡ºï¼š
# # åˆåŒå¯¹æ¯”æŠ¥å‘Š
# 
# ## é‡‘é¢
# âŒ åˆ é™¤: 100ä¸‡å…ƒ
# âœ… æ–°å¢: 120ä¸‡å…ƒ
# 
# ## æ—¥æœŸ
# âŒ åˆ é™¤: 2024å¹´1æœˆ1æ—¥
# âœ… æ–°å¢: 2024å¹´3æœˆ1æ—¥
```

---

## ğŸ—ï¸ å®Œæ•´ç³»ç»Ÿæ¶æ„

### ç³»ç»Ÿç»„ä»¶å›¾

```python
# main.py - ä¸»å…¥å£
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
import uvicorn

app = FastAPI(title="åˆåŒå®¡æŸ¥ç³»ç»Ÿ")

# åˆå§‹åŒ–å„ä¸ªæ¨¡å—
document_parser = DocumentParser()      # æ–‡æ¡£è§£æ
contract_ner = ContractNER()           # å®ä½“è¯†åˆ«
clause_classifier = ClauseClassifier()  # æ¡æ¬¾åˆ†ç±»
reviewer = RAGContractReviewer()       # AIå®¡æŸ¥
comparator = ContractComparator()      # åˆåŒå¯¹æ¯”

@app.post("/api/upload")
async def upload_contract(file: UploadFile = File(...)):
    """ä¸Šä¼ åˆåŒæ–‡ä»¶"""
    # 1. ä¿å­˜æ–‡ä»¶
    file_path = f"uploads/{file.filename}"
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # 2. è§£ææ–‡æ¡£
    if file.filename.endswith('.pdf'):
        text = document_parser.parse_pdf(file_path)
    elif file.filename.endswith('.docx'):
        text = document_parser.parse_word(file_path)
    else:
        return JSONResponse({"error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}, status_code=400)
    
    # 3. ä¿å­˜åˆ°æ•°æ®åº“
    contract_id = save_to_db(file.filename, text)
    
    return {
        "contract_id": contract_id,
        "filename": file.filename,
        "text_length": len(text)
    }

@app.post("/api/review/{contract_id}")
async def review_contract(contract_id: str):
    """å®¡æŸ¥åˆåŒ"""
    # 1. ä»æ•°æ®åº“è·å–åˆåŒ
    contract = get_contract_from_db(contract_id)
    
    # 2. æå–å®ä½“
    entities = contract_ner.extract_entities(contract['text'])
    
    # 3. åˆ†ç±»æ¡æ¬¾
    clauses = clause_classifier.split_and_classify(contract['text'])
    
    # 4. AIå®¡æŸ¥
    review_result = reviewer.review_contract(contract['text'])
    
    # 5. é£é™©è¯„åˆ†
    risk_score = calculate_risk_score(review_result)
    
    return {
        "contract_id": contract_id,
        "entities": entities,
        "clauses": clauses,
        "review": review_result,
        "risk_score": risk_score
    }

@app.post("/api/compare")
async def compare_contracts(old_id: str, new_id: str):
    """å¯¹æ¯”ä¸¤ä¸ªåˆåŒ"""
    # 1. è·å–åˆåŒ
    old_contract = get_contract_from_db(old_id)
    new_contract = get_contract_from_db(new_id)
    
    # 2. æ–‡æœ¬å¯¹æ¯”
    text_diff = comparator.compare_contracts(
        old_contract['text'],
        new_contract['text']
    )
    
    # 3. è¯­ä¹‰å¯¹æ¯”
    semantic_diff = SemanticComparator().compare_clauses_semantic(
        old_contract['clauses'],
        new_contract['clauses']
    )
    
    # 4. ç»“æ„åŒ–å¯¹æ¯”
    key_info_diff = StructuredComparator().compare_key_info(
        old_contract['text'],
        new_contract['text']
    )
    
    return {
        "text_diff": text_diff,
        "semantic_diff": semantic_diff,
        "key_info_diff": key_info_diff
    }

@app.get("/api/report/{contract_id}")
async def generate_report(contract_id: str):
    """ç”Ÿæˆå®¡æŸ¥æŠ¥å‘Š"""
    # ç”ŸæˆPDFæŠ¥å‘Š
    pdf_path = generate_pdf_report(contract_id)
    
    return FileResponse(pdf_path)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ğŸ“¦ å¼€æºé¡¹ç›®æ¨è

### 1. æ–‡æ¡£è§£æ

```bash
# Unstructured - ä¸‡èƒ½æ–‡æ¡£è§£æ
pip install unstructured
pip install "unstructured[pdf]"

# PyMuPDF (fitz) - é«˜æ€§èƒ½PDFè§£æ
pip install PyMuPDF

# pdfplumber - PDFè¡¨æ ¼æå–
pip install pdfplumber

# python-docx - Wordæ–‡æ¡£
pip install python-docx
```

### 2. OCRè¯†åˆ«

```bash
# PaddleOCR - æœ€å¼ºå¼€æºOCRï¼ˆæ”¯æŒä¸­æ–‡ï¼‰â­
pip install paddleocr

# Tesseract - ç»å…¸OCR
pip install pytesseract

# EasyOCR - ç®€å•æ˜“ç”¨
pip install easyocr
```

### 3. NLPå¤„ç†

```bash
# Transformers - Hugging Face
pip install transformers

# spaCy - NLPå·¥å…·åŒ…
pip install spacy
python -m spacy download zh_core_web_sm

# LTP - å“ˆå·¥å¤§è¯­è¨€æŠ€æœ¯å¹³å°ï¼ˆä¸­æ–‡ï¼‰
pip install ltp
```

### 4. å¤§æ¨¡å‹

```bash
# LangChain - LLMåº”ç”¨æ¡†æ¶
pip install langchain langchain-openai

# LlamaIndex - RAGæ¡†æ¶
pip install llama-index

# OpenAI
pip install openai
```

### 5. å‘é‡æ•°æ®åº“

```bash
# Chroma - è½»é‡çº§å‘é‡æ•°æ®åº“
pip install chromadb

# Milvus - ç”Ÿäº§çº§å‘é‡æ•°æ®åº“
pip install pymilvus

# Faiss - Facebookçš„å‘é‡æœç´¢
pip install faiss-cpu  # æˆ– faiss-gpu
```

### 6. æ–‡æœ¬å¯¹æ¯”

```bash
# difflib - Pythonå†…ç½®

# python-Levenshtein - ç¼–è¾‘è·ç¦»
pip install python-Levenshtein

# diff-match-patch - Googleçš„Diffç®—æ³•
pip install diff-match-patch
```

---

## ğŸ¯ å®Œæ•´æŠ€æœ¯æ ˆæ¸…å•

### åç«¯æŠ€æœ¯æ ˆ

| åˆ†ç±»         | æŠ€æœ¯                 | ç”¨é€”       | å¿…é€‰ |
| ------------ | -------------------- | ---------- | ---- |
| **Webæ¡†æ¶**  | FastAPI/Django       | APIæœåŠ¡    | âœ…    |
| **æ–‡æ¡£è§£æ** | PyPDF2/pdfplumber    | PDFè§£æ    | âœ…    |
|              | python-docx          | Wordè§£æ   | âœ…    |
|              | Unstructured         | é€šç”¨è§£æ   | â­    |
| **OCR**      | PaddleOCR            | æ‰«æä»¶è¯†åˆ« | âœ…    |
| **NLP**      | Transformers         | é¢„è®­ç»ƒæ¨¡å‹ | âœ…    |
|              | spaCy                | NLPå·¥å…·    | â­    |
| **å¤§æ¨¡å‹**   | OpenAI API           | GPT-4      | âœ…    |
|              | Qwen                 | å¼€æºæ–¹æ¡ˆ   | â­    |
| **RAG**      | LangChain            | RAGæ¡†æ¶    | âœ…    |
|              | Chroma/Milvus        | å‘é‡æ•°æ®åº“ | âœ…    |
| **æ•°æ®åº“**   | PostgreSQL           | å…³ç³»æ•°æ®åº“ | âœ…    |
|              | MongoDB              | æ–‡æ¡£æ•°æ®åº“ | â­    |
|              | Redis                | ç¼“å­˜       | âœ…    |
| **å¯¹æ¯”**     | difflib              | æ–‡æœ¬å¯¹æ¯”   | âœ…    |
|              | SentenceTransformers | è¯­ä¹‰å¯¹æ¯”   | â­    |

### å‰ç«¯æŠ€æœ¯æ ˆ

| åˆ†ç±»         | æŠ€æœ¯              | ç”¨é€”       |
| ------------ | ----------------- | ---------- |
| **æ¡†æ¶**     | React/Vue         | UIæ¡†æ¶     |
| **ç»„ä»¶åº“**   | Ant Design        | UIç»„ä»¶     |
| **PDFé¢„è§ˆ**  | PDF.js            | æ–‡æ¡£é¢„è§ˆ   |
| **Diffå±•ç¤º** | react-diff-viewer | å¯¹æ¯”å±•ç¤º   |
| **ç¼–è¾‘å™¨**   | Monaco Editor     | æ–‡æœ¬ç¼–è¾‘   |
| **å›¾è¡¨**     | ECharts           | æ•°æ®å¯è§†åŒ– |

---

## ğŸ’¡ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå®Œæ•´çš„åˆåŒå®¡æŸ¥æµç¨‹

```python
class ContractReviewSystem:
    def __init__(self):
        self.parser = DocumentParser()
        self.ner = ContractNER()
        self.reviewer = RAGContractReviewer()
    
    def full_review(self, file_path):
        """å®Œæ•´å®¡æŸ¥æµç¨‹"""
        print("1ï¸âƒ£ è§£ææ–‡æ¡£...")
        text = self.parser.parse_pdf(file_path)
        
        print("2ï¸âƒ£ æå–å…³é”®ä¿¡æ¯...")
        entities = self.ner.extract_entities(text)
        
        print("3ï¸âƒ£ AIå®¡æŸ¥...")
        review = self.reviewer.review_contract(text)
        
        print("4ï¸âƒ£ ç”ŸæˆæŠ¥å‘Š...")
        report = self.generate_report(text, entities, review)
        
        return report
    
    def generate_report(self, text, entities, review):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        return {
            'summary': {
                'parties': entities['ç”²æ–¹'] + entities['ä¹™æ–¹'],
                'amount': entities['é‡‘é¢'],
                'dates': entities['æ—¥æœŸ']
            },
            'review': review,
            'recommendations': self.extract_recommendations(review)
        }

# ä½¿ç”¨
system = ContractReviewSystem()
report = system.full_review("åˆåŒ.pdf")
print(json.dumps(report, ensure_ascii=False, indent=2))
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### æ¨èé¡¹ç›®

1. **LegalBERT** - æ³•å¾‹é¢†åŸŸBERTæ¨¡å‹
   - https://github.com/thunlp/LegalBERT

2. **LaWGPT** - ä¸­æ–‡æ³•å¾‹å¤§æ¨¡å‹
   - https://github.com/pengxiao-song/LaWGPT

3. **æ™ºæµ·-å½•é—®** - æ³•å¾‹å¯¹è¯æ¨¡å‹
   - https://github.com/zhihaiLLM/wisdomInterrogatory

### è®ºæ–‡

- "Legal Judgment Prediction" (æ³•å¾‹åˆ¤å†³é¢„æµ‹)
- "Contract Understanding" (åˆåŒç†è§£)
- "Legal Information Extraction" (æ³•å¾‹ä¿¡æ¯æŠ½å–)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ€å°å¯ç”¨ç‰ˆæœ¬ï¼ˆMVPï¼‰

```python
# 1. å®‰è£…ä¾èµ–
"""
pip install pdfplumber
pip install openai
pip install fastapi uvicorn
"""

# 2. æœ€ç®€å®ç°
from fastapi import FastAPI, UploadFile
import pdfplumber
from openai import OpenAI

app = FastAPI()
client = OpenAI(api_key="your_key")

@app.post("/review")
async def review(file: UploadFile):
    # 1. æå–PDFæ–‡å­—
    with pdfplumber.open(file.file) as pdf:
        text = "\n".join([p.extract_text() for p in pdf.pages])
    
    # 2. GPT-4å®¡æŸ¥
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{
            "role": "user",
            "content": f"è¯·å®¡æŸ¥ä»¥ä¸‹åˆåŒï¼ŒæŒ‡å‡ºé£é™©ç‚¹ï¼š\n\n{text}"
        }]
    )
    
    return {"review": response.choices[0].message.content}

# 3. è¿è¡Œ
# uvicorn main:app --reload
```

åªéœ€50è¡Œä»£ç ï¼Œå°±èƒ½å®ç°åŸºç¡€çš„åˆåŒå®¡æŸ¥åŠŸèƒ½ï¼

---

## æ€»ç»“

åˆåŒå®¡æŸ¥AIé¡¹ç›®çš„æ ¸å¿ƒæŠ€æœ¯æ ˆï¼š

```
ğŸ“„ æ–‡æ¡£å±‚ï¼šPyPDF2/pdfplumber + PaddleOCR
ğŸ§  NLPå±‚ï¼šTransformers + spaCy + å®ä½“è¯†åˆ«
ğŸ¤– AIå±‚ï¼šGPT-4/Qwen + RAG (LangChain + Chroma)
ğŸ“Š å¯¹æ¯”å±‚ï¼šdifflib + SentenceTransformers
ğŸŒ æœåŠ¡å±‚ï¼šFastAPI + PostgreSQL + Redis
ğŸ¨ å‰ç«¯å±‚ï¼šReact + Ant Design + PDF.js
```

**å»ºè®®å­¦ä¹ è·¯å¾„**ï¼š
1. å…ˆç”¨OpenAI APIå®ç°åŸºç¡€ç‰ˆæœ¬
2. å†åŠ å…¥æ–‡æ¡£è§£æå’ŒNER
3. ç„¶ååŠ å…¥RAGæé«˜å‡†ç¡®ç‡
4. æœ€åä¼˜åŒ–æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ

æœ‰é—®é¢˜éšæ—¶é—®æˆ‘ï¼ğŸš€

